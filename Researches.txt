PMC ID: 12380472
In daily life, people often form impressions of others and make judgments based on highly limited information, which is referred to as ‘thin-slicing’ in psychology (Ambady and Rosenthal 1992). Such an ability demonstrates how people use fast and automatic processes to make decisions, such as interpreting nonverbal cues like facial expressions, gestures, and tone of voice during interactions. For example, reading a text about a person’s major life events can predict their personality (Pennebaker and King 1999), and simply observing someone’s room can accurately judge their personality (Gosling et al. 2002). Moral judgments can be made from brief visual scenes (De Freitas and Hafri 2024), and after watching a 60-second video of a social interaction, one can predict the other person’s socioeconomic status (Kraus and Keltner 2009).
Among others, facial cues are particularly integral to social inference, serving as a rich source of insight into an individual’s traits (Zebrowitz et al. 2002, Frith and Frith 2012). Previous studies have shown that, in less than 100ms, one can infer how trustworthy someone is (Willis and Todorov 2006), predict a company’s financial performance by looking at the CEO’s face (Rule and Ambady 2008b), accurately assess someone’s sexual orientation (Rule and Ambady 2008a), and predict the outcomes of political candidates in elections (Todorov et al. 2005).
Numerous psychological literatures highlight the significance of intuitive social inferences, in which individuals are tasked with inferring affective states or psychological traits of targets. These tasks encompass estimating transient emotional states as interpreted through facial expressions (Frith and Frith 2012), understanding intentions of approach and avoidance (Jones and Kramer 2021), and discerning psychological traits such as trustworthiness (Van’t Wout and Sanfey 2008, Chwe and Freeman 2023), cooperativeness (Tognetti et al. 2013), intelligence (Zebrowitz et al. 2002), and preferences (North et al. 2010, 2012, Kang et al. 2013, Eggleston et al. 2015, Pollmann and Scheibehenne 2015, Vijayakumar et al. 2021). These intuitive social inferences, based on facial information, are integral components of understanding others, including empathy and the theory of mind (Quesque and Rossetti 2020), yet the exact neural mechanisms behind these instinctual inferences require further exploration.
What factors determine the accuracy of predicting others’ traits based on limited information, such as a face? Research conducted to answer this question so far suggests that higher emotional recognition ability (Jaksic and Schlegel 2020) and intuitive over analytical thinking (Albrechtsen et al. 2009) are associated with higher accuracy in thin-slicing. Compared to the vast psychological research on thin-slicing, studies on the neurological mechanisms related to this phenomenon are relatively scarce, especially regarding the biological factors that differentiate individuals with high and low accuracy in thin-slicing. Research that identifies the neurological characteristics underlying individual differences in thin-slicing accuracy is expected to provide important insights into distinguishing innate from acquired factors in this phenomenon.
Our laboratory has previously reported on the significant role of the dorsomedial prefrontal cortex (dmPFC) communicating with the temporoparietal junction (TPJ) in predicting others’ preferences based on facial features and the associated individual differences in accuracy (Kang et al. 2013, Park et al. 2018). Prior research in social cognition suggests that the dmPFC, in concert with the TPJ, forms the core of the mentalizing network (Schnell et al. 2011, Dvash and Shamay-Tsoory 2014). Meta-analyses indicate that dmPFC activities are engaged when observing others receiving painful stimuli (Lamm et al. 2011), estimating others’ thoughts through perspective-taking, and inferring their emotional states (Schurz et al. 2014, 2021), emphasizing its role in abstract representation based on external information for generating appropriate affective and cognitive behaviours (Kim 2020). Beyond mentalizing studies, the dmPFC is involved in generating new actions based on prediction errors and learning abstract rules (Seo et al. 2014) and representing other-regarding values for prosocial decisions (Sul et al. 2015). Increased dmPFC activity during the observation of social scenarios correlates with a higher frequency of social interactions (Powers et al. 2016). Similarly, dmPFC activities are instrumental in predicting others’ behaviour (Amodio and Frith 2006) and showed a positive correlation with the accuracy of preference estimation based on facial appearance (Kang et al. 2013).
Multivariate pattern analysis (MVPA) is a statistical technique used to analyse brain imaging data. Unlike traditional univariate analysis, which examines activity of one voxel at a time, MVPA analyzes patterns of activity across multiple voxels simultaneously, making it more sensitive to detecting subtle differences between psychological states (Norman et al. 2006). Supporting evidence shows that multivariate analyses reveal unique activity patterns in brain regions that are commonly activated across conditions in univariate analyses (Woo et al. 2014, Krishnan et al. 2016, Wake and Izuma 2017, Kim and Kim 2021). Considering the differences between these two methods, it is necessary to determine whether the role of the dmPFC, linked to individual differences in the accuracy of predicting others’ preferences, remains valid in both univariate and multivariate analyses, or if there are specific differences between the two methods related to these individual differences.
In this study, we delve into the neural mechanisms underlying individual variations in preference estimation accuracy. First, we aimed to replicate the findings regarding the role of the dmPFC in accurately evaluating other’s preferences, as previously reported by Kang et al. (2013). Secondly, we opted to capitalize on recent advances in MVPA techniques, renowned for their capability to map hidden neural representations of psychological traits and motivations that could not be captured by conventional univariate approaches (Kragel et al. 2021, Contreras-Huerta et al. 2023). To achieve this, we employed a preference estimation task, where participants were shown an image of a person followed by an image of a food or movie poster (Kang et al. 2013) and asked to infer the preference of the person to the image solely based on their facial appearances. Our study leveraged inter-subject representational similarity analysis (van Baar et al. 2019) to identify the specific brain regions where activity patterns are correlated with individual variability in the accuracy of preference estimation.
A total of 39 healthy female participants, aged between 22 and 44 (all females; mean age = 30.82), were recruited for this study. The decision to recruit only female participants was based on prior research (Carney et al. 2007), which suggests that female participants tend to make more accurate thin-slice judgments compared to male participants. By selecting a single gender cohort, we sought to minimize potential gender effects. All participants were right-handed, had no visual impairments, and reported no history of psychiatric or neurological conditions. However, due to artefacts in neuroimaging data, one participant’s data was excluded. Thus, final analyses were conducted on 38 participants (age range: 22–44, mean age = 30.56). To determine whether our study had sufficient statistical power, we conducted a Bayesian power analysis. Unlike traditional frequentist power analysis, which assumes a fixed effect size, Bayesian approaches incorporate empirical uncertainty to provide a more robust and realistic assessment of power (Du and Wang 2016, Kruschke and Liddell 2018). Based on our observed effect size (r = .566) and a final sample size (N = 38), the probability that the true effect size exceeds r = .34 is 95.27%. This threshold of r = .34 was derived from a transformation of the effect size reported in Kang et al. (2013). These results suggest that our study was well-powered to detect meaningful effects within the expected range, thereby confirming the robustness of the hypothesized relationship. All participants provided informed consent, in accordance with the guidelines set by the Institutional Review Board of Korea University. These participants performed a facial expression recognition task prior to this task, and these data have already been published elsewhere (Kim et al. 2022).
Upon arrival, participants’ eligibility for MRI scanning was verified. Before scanning, participants were given instructions and completed a couple of practice trials to familiarize themselves with the experimental paradigm. The primary task was the preference estimation paradigm adapted from Kang et al. (2013), designed to investigate neural mechanisms associated with inferring preferences (e.g., for movie posters or food items) based on facial photographs of targets. The photos of targets were collected from an independent group of eight individuals (four males, four females; all Korean) who had provided their preferences for five distinct food items and five distinct movie stimuli.
Following the methodology established by Kang et al., we selected food and movie posters as stimuli to best allow participants to estimate others’ preferences accurately. In their study, various item categories were evaluated, including movies, books, bags, shoes, and foods, to determine which categories would best facilitate accurate preference estimation. Items were selected based on preference ratings, focusing on those with intermediate levels of preference and high variability, to minimize overlap between general population preferences and individual target preferences. Their findings indicated that participants could reliably estimate preferences for movies and foods, while accuracy was lower for other categories such as books and bags. This suggests that, in certain domains, participants were able to accurately estimate others’ preferences, even with very brief exposure to limited information, such as facial appearance. Given these considerations, we adopted food and movie posters as stimuli in our study, aligning with prior research demonstrating that these categories are suitable for assessing accuracy in estimating others’ preferences.
To enhance participants’ engagement in estimating the targets’ preferences, they were informed that monetary incentives would be given based on their relative performance. Notably, no participant had prior familiarity with any of the photo targets. Before the MRI scanning, participants also provided facial photographs for later use in their own personal preference estimation condition.
During the single MRI session, which lasted approximately 18min, participants performed the preference estimation task (Fig. 1), with trials presented in event-related paradigms. Each trial began with a 3-second display of a facial photo (either of the participant self or a target), followed by a jittered fixation interval lasting 2–4seconds. Subsequently, an item was displayed, and participants provided their own preference (self-trial) or guessed the targets’ preference (target-trial) for the depicted item using a 4-point Likert scale (from ‘strongly dislike’ to ‘strongly like’) within a 3-second response window. Immediately after the response, participant’s choice was displayed on the screen for 0.5seconds. It is important to note that the ‘participant’s choice’ here refers to the predictions and preference inferences made solely by the participants, rather than feedback on the actual preferences of the targets. No information about the actual preferences of the 8 targets was provided to the participants at any point in the study. Therefore, there was no chance for participants to learn or adjust their predictions based on feedback. Each prediction was made independently on each trial, based solely on the participants’ judgement without any external information influencing subsequent estimations. The MRI session comprised 10 self-trials and 80 target-trials (i.e., facial photo (9) × item (10)). Trials were presented in a pseudo-random order. In the post-scan session, participants were debriefed about the study’s objectives and compensated with 40,000 KRW (approximately $38).
Task design. In the preference estimation task, the participants were asked to infer the target’s preference for a given item. A target was presented during the face phase for 3 s in the target-trials, and then an item was displayed. During the item phase, the participants had to perceive an item and rate how much the target would favour this specific item on a 4-point scale. When the participants answered, their response was shown on the screen for 0.5 s.
We measured preference estimation accuracy by assessing the proportion of trials where participants’ estimations precisely matched the targets’ stated preferences. Only exact matches were considered correct when calculating the accuracy score. For instance, if a target rated an item as 3 (like) and a participant estimated it as 2 (dislike), it was marked incorrect. Similarly, if a target rated an item as 3 (like) and the estimation was 4 (highly like), despite both being positive, the trial was still deemed incorrect. Overall, the accuracy of participants’ estimation was significantly above the chance level (0.25) on average (Fig. 2), meaning that participants can infer others’ preference from their faces better than random guessing (M = 0.31, t(37) = 5.91, P < .001). Furthermore, we conducted supplementary analyses covering various aspects, including baseline preferences for movie and food stimuli (Supplementary Fig. S1), preferences by target gender (Supplementary Fig. S2), participants’ accuracy rates for each target (Supplementary Fig. S3), accuracy differences between movie and food stimuli (Supplementary Fig. S4), and performance comparisons between high and low accuracy groups (Supplementary Fig. S5).
Accuracy scores (percentage) of each participant. Participants scored above the chance level (dot line) on average (M = 0.31, t(37) = 5.91, P < .001).
All the neuroimaging data were collected from a 3T Siemens Trio MRI scanner (MAGNETOM Trio, A Tim System; Siemens AG, Erlangen, Germany) with a 12-channel head coil located at the Korea University Brain Imaging Center. We acquired functional images using gradient echo-planar images (EPI) with Blood Oxygenation Level-Dependent contrast (TR = 2000ms; TE = 30ms; flip angle = 90°; FOV = 240mm; 3 × 3 × 3mm in-plane resolution; 80 × 80 matrix size; and 36 slices with no gap with interleaved sequence), and high-resolution structural images (TR = 1900ms; TE = 2.52ms; flip angle = 9°; 1 × 1 × 1mm in-plane resolution; and 256 × 256 matrix size). The experiment task was presented through an MR-compatible LCD monitor mounted on a head coil (refresh rate: 60Hz; display resolution: 640 × 480 pixels; viewing angle: 30°) operating on MATLAB 2009b with Cogent 2000 stimulus presentation software.
The functional data were preprocessed by using the default preprocessing pipelines of the CONN toolbox 2018b (www.nitrc.org/projects/conn, RRID: SCR_009550; Whitfield-Gabrieli and Nieto-Castanon 2012). The images were first realigned and unwarped, centred to (0, 0, 0) coordinates, and slice-time corrected in sequential order. The resulting images were then spatially normalized to the standard Montreal Neurological Institute 152 reference template resampled to 2mm isotropic voxels. Finally, the normalized images were then smoothed with an 8-mm full-width at half-maximum Gaussian kernel.
In the first-level analyses, we processed the neuroimaging data utilizing SPM12 software (Wellcome Department of Imaging Neuroscience, London, United Kingdom). Individual participant data were modelled using a general linear model (GLM), which incorporated five regressors: (i) the self-trial during the face phase, (ii) the target-trial during the face phase, (iii) the self-trial during the item phase, (iv) the target-trial during the item phase, and (v) button response onset. Additionally, we accounted for head-motion artefacts by including six head-motion regressors as covariates of no interest.
To identify brain regions associated with individual preference estimation performance, we performed a linear regression analysis, incorporating the contrast images derived from target-trials versus self-trials during the item phase and individual accuracy scores as covariates. A parallel analysis was applied to the data obtained during the face phase.
To precisely examine our primary hypothesis—that there exists a correlation between the activation of the dmPFC and the participants’ accuracy scores, as suggested by previous research utilizing the same paradigm (Kang et al. 2013)—we utilized a dmPFC binary mask. This mask was derived from a meta-analysis by de la Vega et al. (2016), which segmented the medial prefrontal cortex (mPFC) into nine distinct subregions based on each region’s functional coactivation maps. Our methodological choice was guided by our focused research question, leading us to opt for a univariate approach exclusively targeting the dmPFC subregion [A1], excluding other mPFC subregions from our analysis. This approach was not only aimed at confirming previous findings but also at providing a rigorous test of the role this particular subregion plays in the task at hand, thereby advancing our understanding of the functional specialization within the mPFC in the context of preference estimation. In addition to our primary objective, we sought to determine if other subregions of the mPFC played a role in the preference inference process. To this end, we conducted further analysis using an mPFC binary mask that encompasses dmPFC [A1], pgACC [A2], and vmPFC [A3].
The motivation to accurately infer others’ preferences can conflict with the motivation to express one’s own preferences. According to our model, such conflicts between motivation embedded in the vmPFC appear to be resolved through close communication with the dmPFC, leading to value adjustment (Kim 2020). Based on this hypothesis, in addition to the GLM-based fMRI data analysis, we ran a generalized psychophysiological interaction (gPPI) analysis (McLaren et al. 2012), which is particularly useful in examining task-dependent functional connectivity, to test if individuals with high accuracy in predicting others’ preferences would exhibit stronger functional connectivity between the vmPFC and dmPFC in the ‘other’ condition compared to the ‘self’ conditions. At an individual level, a voxel of interest (VOI) was extracted with the 5mm sphere around the peak (coordinates: x = 0, y = 38, z = −10) of the vmPFC cluster found in the multiple regression analysis. We utilized this VOI as a seed region. Time series were extracted from the vmPFC and served as the physiological variable, reflecting the neural activity in that region during the task. The contrasts for target- versus self-trials during the item phase served as the psychological variable. Then, the physiological and psychological variables were multiplied to create the PPI term. Finally, the individual accuracy scores were regressed with the contrast images of PPI term.
Our particular interest was the functional coupling between the mPFC subregions, with a specific focus on the vmPFC and dmPFC. To implement this analysis, we used an dmPFC binary mask [A1], consistent with the GLM-based approach above. This allowed us to explore whether the contextual connectivity strength within the mPFC increased during target-trials compared to the self-trials among the participants with higher accuracy scores.
We utilized MVPA to identify brain regions that encapsulate inter-subject variability during the estimation of preferences based on facial feature processing. In our study, we employed inter-­subject RSA (IS-RSA; van Baar et al. 2019) to identify regions associated with variability in preference estimation accuracy. The main question was which brain regions exhibited similar activation patterns among participants with comparable accuracy scores in estimating others’ preferences.
First, we segmented the entire brain into functionally relevant regions using a predefined 200-parcel map from the Neurosynth database (https://neurosynth.org/). This map organizes the global brain atlas based on meta-analytic functional coactivation patterns. In our analysis, 183 parcels were included, omitting seventeen parcels due to their absence in our scan scope. Given prior research indicating distinct statistical properties between univariate and multivariate approaches (Jimura and Poldrack 2012, Coutanche 2013, Davis et al. 2014), we extended the ROI map initially used for univariate analysis. Importantly, MVPA captures heightened sensitivity to within-subject voxel variability but displays diminished sensitivity to inter-subject mean activation variability compared to univariate analyses (Davis et al. 2014). To understand individual differences in preference accuracy, we computed a behavioural dissimilarity matrix by calculating the absolute difference in accuracy scores among all participant dyads.
Subsequently, we constructed an inter-subject dissimilarity matrix for each of the 183 neural activation map parcels to assess inter-subject dissimilarity in mean activity patterns during the inference of the target’s preference. We used the identical contrast map as in the univariate analyses for inter-subject representational similarity analysis (IS-RSA). Of particular interest were the target-­trials compared to self-trials during the item and face phases. To identify brain regions where neural representations corresponded with behavioural inclination, we computed a nonparametric Kendall’s tau-a correlation between the parcel dissimilarity matrices and the behavioural ones (Nili et al. 2014). The significance of tau-a was validated through permutation testing. Specifically, while retaining the neural Representational Dissimilarity Matrix (RDM), we randomized the behavioural RDM 10,000 times to create a null distribution. To account for multiple comparisons, we adjusted the P-values using an FDR correction, and P-values below .01 post-adjustment indicated a significant correlation between inter-subject behavioural patterns of preference estimation accuracy and neural representation patterns of the parcel.
Our primary objective in the univariate analysis was to replicate the findings of the previous study (Kang et al. 2013), regarding the involvement of the dmPFC in accurately estimating another person’s preferences with limited information. From the mPFC subregions (de la Vega et al. 2016), we included dmPFC area [A1] as binary mask for a regression analysis. Individual contrast maps of target- vs. self- trials were regressed against individual accuracy scores as a covariate. As predicted, during the item phase, we found a significant correlation in the dmPFC: a right cluster (peak coordinates: x = 8, y = 46, z = 50, small volume correction (SVC) corrected, PSVC-peakFWE = 0.028) and a left cluster (peak coordinates: x = -10, y = 44, z = 50, small volume correction (SVC) corrected, PSVC-peakFWE = 0.038) (Fig. 3). This suggests that individuals with higher accuracy scores exhibited greater activity within these clusters during the evaluation of items for the targets compared to themselves. No significant correlation was found during the face phase.
Univariate results. The mPFC activity predicts the estimation accuracy score. (A) The neural activations of the dmPFC (peak coordinates: x = 8, y = 46, z = 50) were positively correlated with how accurately participants guessed the targets’ actual preferences (SVC corrected, PSVC-peakFWE = 0.028) during the item phase. (B) The vmPFC (peak coordinates: x = 0, y = 38, z = -10) activity also positively correlated with the accuracy score (SVC corrected, PSVC-clusterFWE = 0.028) during the item phase. The scatterplot of the mPFC activations and the accuracy score is presented on the right side. The shaded area indicates the 95% confidence interval.
Although the dmPFC was the primary ROI of the regression analysis, we performed additional analysis to explore whether other regions of the mPFC was engaged in accurately inferring other’s preferences. Therefore, we included other mPFC subregions, including dmPFC [A1], pgACC [A2], and vmPFC [A3], as a mask. The result revealed a significant correlation in the vmPFC (peak coordinates: x = 0, y = 38, z = -10, small volume correction (SVC) corrected, PSVC-clusterFWE = 0.028) during the item phase. Conversely, no significant correlation was found during the face phase. In addition, no brain regions showed significant results after correction for multiple comparisons at the whole-brain level.
Multiple regression analysis revealed that not only did dmPFC but also vmPFC regions activated more strongly as the participants’ accuracy score increased. To examine the underlying rationale, we delved into the neural connectivity within mPFC subregions in relation to individual variations in preference estimation accuracy. This scrutiny pinpointed a significant cluster in the dmPFC (peak coordinates: x = 4, y = 36, z = 52, small volume correction (SVC) corrected, PSVC-peakFWE = 0.034, Fig. 4). Specifically, functional connectivity between the vmPFC and dmPFC intensified among the participants with higher accuracy when discerning targets’ preferences compared to when expressing their own preferences.
gPPI result. The functional connectivity of the vmPFC and dmPFC is linked to higher accuracy scores in estimating targets’ preferences. A heightened psychophysiological interaction with vmPFC activity was observed in the dmPFC (peak coordinates: x = 4, y = 36, z = 52, small volume correction (SVC) corrected, PSVC-peakFWE = 0.034) during target-trials compared to self-trials. The scatterplot on the right side illustrates the relationship between the strength of connectivity between the vmPFC and dmPFC and individual accuracy scores in estimating preferences, with the shaded area indicating the 95% confidence interval.
Significant inter-subject representational similarity effects were found in four brain parcels, during the item phase, including the ventral anterior insula (vAI), the dorsal anterior insula (dAI), and the pregenual anterior cingulate cortex (pgACC) (Fig. 5). This implies that the inter-subject dissimilarity of neural activity patterns during the estimation of others’ preferences corresponded to the inter-subject distance pattern of accuracy score. Conversely, no significant is-RSA effect was yielded during the face phase. These results indicate that individuals with similar accuracy scores shared similar multi-voxel patterns only during the item phase, not when processing the facial features of the target.
Multivariate results. Inter-subject representational similarity analysis indicates that participants with similar accuracy score exhibited similar neural patterns in brain regions including the pgACC and insula.
Notably, the pgACC parcel identified through the is-RSA did not overlap with the vmPFC cluster obtained through the univariate approach (Fig. 6). Despite their close proximity, the absence of overlap suggests that both analytical methods can yield analogous yet distinctly separate outcomes. When using continuous measures, such as distance from the correct rating and its inverse, only the insula showed significant association with preference estimation in multivariate analysis, with no significant regions in univariate analysis. This suggests that exact accuracy may be closely linked to specific neural mechanisms, highlighting the role of precision in social inference tasks.
Overlay of results. The vmPFC cluster (green) identified through the univariate approach is combined with the pgACC parcel (yellow) from the multivariate approach.
This study investigated the neural underpinnings associated with individual differences in the ability to infer others’ preferences based solely on facial cues. Consistent with the findings of Kang et al. (2013), considerable variability emerged among participants in their accuracy of estimating others’ preferences, with some individuals demonstrating adeptness while others exhibit less precision. This behavioural variation corresponded with distinct patterns observed in the neuroimaging data collected during the task. Univariate analysis reaffirmed the role of the dmPFC in preference estimation, particularly highlighting its activity during the item phase as indicative of the accuracy in inferring others’ preferences. Furthermore, multivariate analysis, particularly the results from IS-RSA, showed correlations between participants’ estimation accuracy patterns and neural activity patterns in the pgACC, ventral AI, and dorsal AI during the item phase. Collectively, these findings underscore the central role of the mPFC and insula in the process of estimating others’ preferences.
Understanding others’ minds involves recognizing their thoughts and emotions, even when these mental states are not directly observable. Traditionally, two key concepts, Theory of Mind (ToM) and empathy, have framed these social cognitive capacities. However, a recent meta-analysis study suggests that these terms have functioned as ‘umbrella terms’ encompassing various processes (Schaafsma et al. 2015, Zaki 2017). This has led to inconsistent use of terminology, with different terms being used to describe similar process and vice versa (Schurz et al. 2021). Proposals have been made for a coherent hierarchical model of social cognitive processes, positioning empathy and ToM as higher-order processes. The discussion also advocates for breaking down these processes into smaller, interdependent components through ‘deconstruction’ (Schaafsma et al. 2015), warranting further exploration of these building blocks. In response, our research aims to contribute as an integral component of this framework. Specifically, our study investigates the cognitive mechanisms involved when individuals infer others’ preferences through minimal facial cues combined with contextual information—item cues. Furthermore, we hypothesize that this cognitive building block is associated with neural activity in the mPFC and the insula.
Various subregions of the mPFC, operating in coordination, play a pivotal role in social valuation. A recent model proposes a hierarchical organization of mPFC subregions, with the more dorsal region incorporating additional external sensory information from the environment to regulate the more ventral region, which processes intuitive social values based on internal bodily signals (Kim 2020).
Consistent with the previous study (Kang et al. 2013), our study reaffirmed the role of the dmPFC in estimating others’ preferences. The dmPFC is recognized as a key component of the broader mentalization network, contributing to a wide spectrum of social processes such as reasoning about other’s mental states (Mitchell 2008, Wagner et al. 2012, Wagner et al. 2016), forming impressions of others (Mitchell et al. 2005, Schiller et al. 2009), and predicting social interactions (Powers et al. 2016). Given its role in mentalization, it is reasonable to consider the dmPFC as pivotal in other’s preference estimation. Moreover, the dmPFC integrates information from various brain networks to guide thoughts and actions (Shackman et al. 2011), facilitating adaptive responses in a complex social environment.
Contrary to our initial expectations, the ventral region of the mPFC (vmPFC) also emerged as significant in accurately inferring others’ preferences. In addition, exploratory gPPI analysis revealed heightened functional coupling between the vmPFC and dmPFC in individuals with greater accuracy scores. This suggests that individuals with higher accuracy scores exhibit increased connectivity between these regions when assessing others’ preferences compared to when expressing their own preferences. Drawing from recent theories of morality and altruism (Haidt 2007, Kim 2020), we propose that this phenomenon may stem from an instrumental desire to accurately estimate others’ preferences, driven by a primal instinct for survival and reproduction. In other words, individuals may learn that precise predictions foster favourable impressions, leading to the internalization of this instrumental desire within the vmPFC. This facilitates an automatic and intuitive motivation to seek detailed information for preference estimations. This hypothesis aligns with evidence indicating that the vmPFC is involved in context-independent internalized prosocial valuation (Sul et al. 2015, Jung et al. 2018). Considering previous research indicating the association between the vmPFC and self-centred preferences (Kang et al. 2013), individuals with high accuracy in predicting others’ preferences may demonstrate increased vmPFC-dmPFC communication, integrating additional external information to resolve conflicts between the instrumental desire for predicting others’ preferences and self-centred preferences encoded within the vmPFC (Kim 2020).
In this study, our a priori goal was to uncover the neural patterns associated with accurately estimating others’ preferences. Our results showed that individuals with similar behavioural scores exhibited comparable activity patterns in the pgACC and insula, both crucial for interoception—the perception of internal bodily sensations (Critchley and Harrison 2013, Barrett and Simmons 2015). The pgACC and AI are central components of the neural circuitry linked to predicting bodily reactions or maintaining internal equilibrium, potentially through von Economo neurons (VEN), which facilitate rapid brain-body communication (Allman et al. 2010, Fischer et al. 2016). Although this connection suggests a potential link between sensing our own internal states and accurately inferring others’ preferences, it remains unclear how the interoceptive network contributes to this process of correctly inferring others’ preferences.
The pregenual anterior cingulate cortex (pgACC), in conjunction with the anterior insula (AI), also plays a key role in the processing of social information. The pgACC is actively engaged in tracking others’ motivations (Chang et al. 2013, Apps et al. 2016), differentiating between self and other-oriented information in social interactions (Lockwood and Wittmann 2018), and processing empathy (Xu et al. 2009, Wittmann et al. 2018). The activities of the pgACC were positively linked with the ability to utilize contextual information for inferring others’ emotional states (Kim et al. 2022), and damage to the pgACC can impede social awareness and empathic capabilities (Seeley 2008). The AI, another critical region, plays a significant role in social information processing and is implicated in empathy, compassion, and various interpersonal phenomena (Lamm and Singer 2010). AI integrates multiple bodily signals to simulate internal states of the targets to generate appropriate empathic behaviours (Singer et al. 2009). Difficulties in theory of mind and hypoactivity in AI during face processing have been noted in autism spectrum disorder (Uddin and Menon 2009). In the context of social inference concerning the learning of others’ preferences, both the pgACC and AI contribute to these complex processes (Lau et al. 2020). Furthermore, individual differences in the activities of the pgACC and AI signify the degree of context dependency in estimating other’s affective states (Kim et al. 2022). The extensive involvement of the pgACC and AI in social information processing hints that they may play a coordinating role in predicting preferences in a target individual.
From birth, individuals are deeply intertwined with their primary caregivers for survival, and as they mature, they engage in a dynamic interplay with a broader social environment, shaping and being shaped by it (Atzil et al. 2018). Consequently, individuals develop internal frameworks based on societal norms and the behaviours of those around them. These frameworks shape our ‘sense of should (Theriault et al. 2021),’ guiding us to act in accordance with cultural and social standards (Constant et al. 2019). Conforming to meet others’ expectations can foster a more stable social setting, helping us conserve energy as we navigate the intricacies of social interactions. Previous research has linked the tracking of social norms, aimed at minimizing metabolic costs, to interoceptive processing (Theriault et al. 2021, Sennesh et al. 2022).
Within the framework of interoceptive predictive coding, the brain continually forms and updates models of the external environment and internal organism states (Petzschner et al. 2021, Engelen et al. 2023). Attention plays a pivotal role in modifying these models (Feldman and Friston 2010), prompting individuals to update them in response to prediction errors. Participants who excelled in this task may habitually place significant weight on their social environment, adjusting their generative models more frequently and specifically for each individual encountered. Through consistent attention and adjustment, they likely developed more refined and detailed internal models of others, providing a reliable reference point for more accurate predictions despite task’s requirement to predict the preferences of unfamiliar individuals.
This study suggests that that activation patterns in the mPFC and AI are crucial in accounting for individual differences in thin-slicing ability—the capacity to accurately predict others’ preferences based on briefly presented facial cues. Given that these regions are key components of the interoceptive neural circuitry, it is plausible that constructing an accurate predictive model of others’ preferences—by minimizing the gap between expectations and observed behaviours—requires close communication between the brain and body. Such a refined model could facilitate social interactions, reduce conflicts, and ultimately enhance social adaptability by conserving bodily resources.
Another noteworthy discovery in this study pertains to the neural patterns in the pgACC and insula, which exhibited variations among participants only during the item phase, not the face phase. This suggests that when facial information was presented, no regions showed neural patterns similar to those of the behavioural accuracy score, implying that estimation accuracy was not contingent upon distinct facial information processing alone. Instead, the critical determinant of preference accuracy appeared to be the integration of both facial and item information, with the insula and pgACC emerging as a pivotal region in this integration process. This finding resonates with previous study identifying the AI’s crucial role in cue integration for empathic responses and contextual dependency in ambiguous facial emotion processing (Kim et al. 2022). Conjunction analyses focusing on empathy, interoception, and social cognition have highlighted the AI as a central hub for integrating interoceptive and social information (Adolfi et al. 2017). Moreover, the communication between the pgACC and AI has been implicated in interoceptive prediction (Barrett and Simmons 2015). Consequently, we hypothesize that both the pgACC and insula play pivotal roles in interoception and social cognition, seamlessly integrating internal and external information to construct sophisticated models of the social environment relevant to an individual’s survival goals.
This research particularly focuses on the discrepancy between univariate and multivariate analyses. While the pgACC and insula were identified as significant regions in RSA analyses, they were not found in univariate approaches. MVPA is known for its superior sensitivity in detecting subtle differences between psychological states, whereas univariate analysis is more sensitive to inter-subject variability in mean activation across voxels within a specific ROI (Davis et al. 2014, Kohoutová et al. 2020). In support of this, it has been observed that even within the same brain regions, univariate and multivariate analyses can yield opposing results (Woo et al. 2014, Kim and Kim 2021). It has been established that the magnitude of the BOLD response is sensitive to changes in the excitation-inhibition balance within cortical microcircuits, which involve pyramidal projection neurons interacting with local GABAergic interneurons, possibly reflecting mismatch or prediction error-related feedback signals (Logothetis 2008). From this perspective, the dmPFC activity observed in univariate analysis could be interpreted as reflecting the activity of local GABAergic interneurons responding to mismatches or prediction errors arising from the comparison between internal models and external information in the process of inferring others’ preferences. In contrast, the pgACC and insula activity observed in multivariate analysis may not be directly related to this preference inference process. While multivariate analysis provides valuable insights into the neural representations associated with individual differences, the lack of convergence across different analytical methods highlights the need for further investigation. Moreover, MVPA, originally developed as a predictive tool, may not always be suitable for interpreting brain function and is more complex than univariate analysis, requiring caution (Hebart and Baker 2018). Future research should employ complementary analytical approaches to better understand the neural mechanisms underlying preference estimation.
The activation patterns of the mPFC and AI were correlated with fine-grained accuracy scores, but not with categorical accuracy. Here, categorical accuracy refers to a coarser measure in which ‘like’ and ‘strongly like’ responses are collapsed into a single category, as are ‘dislike’ and ‘strongly dislike.’ This distinction emphasizes that the observed neural correlates may be more strongly linked to precise, fine-grained estimations than to broad categorical classifications.
This discrepancy between fine-grained accuracy scores and categorical accuracy scores may stem from the fact that our behavioural task required participants to choose one out of four options rather than simply indicating like versus dislike. A four-choice task likely demands more extensive information processing and mentalization than a two-choice task, potentially engaging different neural circuits. Even if the same neural circuits are involved, the level and pattern of activation might vary. Future research should investigate whether a like versus dislike task would reveal correlations between the activation patterns of the mPFC and AI would and categorical accuracy. Furthermore, this distinction aligns with hierarchical models of social cognition, which propose that higher-order cognitive processes involve complex and abstract reasoning, whereas lower-order processes rely on faster, heuristic-based decision-making (Schurz et al. 2021). Given that fine-grained judgements require detailed consideration of individuating information rather than broad categorical assumptions, they are more likely to engage neural regions associated with higher-order social inference. In contrast, categorical classification often relies on heuristic processing, allowing for rapid but less nuanced judgments. Our findings, therefore, suggest that the involvement of the mPFC and AI in fine-grained accuracy may reflect the increased cognitive demands of more effortful social inference.
This study has a limitation that the participant pool consisted exclusively of female participants. This choice was made to control for potential gender differences in social cognition and preference estimation, as prior studies have indicated such disparities (Carney et al. 2007). However, this limits the generalizability of our findings to a broader population, including males or mixed-gender groups. Additionally, participants may have inferred the targets’ preferences based on various pieces of information that can be extracted from their faces (e.g., BMI, facial expression). Future research should aim to include a more diverse sample to improve generalizability and examine which specific facial cues contribute to preference estimation and to what extent.
Furthermore, while our Bayesian post hoc power analysis suggests that the study was likely well-powered to detect the observed effects, we acknowledge that post hoc power calculations based on observed data may provide biased estimates of actual power (Heinsberg and Weeks 2022). These results should therefore be interpreted with caution. Nonetheless, we believe that our significant findings provide credible initial evidence for reported effects. In line with recent study (Lengersdorff and Lamm 2025), we suggest that statistically significant results can retain evidential value even when derived from studies that may not meet traditional power thresholds. Future research should conduct a priori power analyses based on the effect sizes observed in the present study to firmly establish the robustness and replicability of these findings.
In conclusion, this study delved into individual variability in accurately estimating the preferences of strangers based on minimal information. Consistent with prior research, participants overall surpassed chance level estimations, demonstrating a wide range of accuracy scores, with the dmPFC playing a significant role. Notable, activity patterns within the interoceptive network, particularly the pgACC and insula, correlated with similar accuracy scores during the item phase. These findings underscore the neural signatures of individual differences in accurately assessing others’ preferences and tailoring estimations to specific targets rather than relying on broad generalizations. We propose that the ability to rapidly infer others’ preferences from sparse information serves as a foundational, lower-order process contributing to the more complex, higher-order processes facilitating social cognition. However, further research is needed to elucidate the precise mechanisms of this process in more detail.
========================================
PMC ID: 12380450
Children’s drawing is a commonly used way of examining children’s cognitive and emotional status [1]. The reason for this is not only because drawing is a pleasurable activity for children [2], but also because its effectiveness in examining children’s psychological activities is considered reliable and has been extensively studied in many fields. For example, from the perspective of individual learning, drawing is used to examine children’s cognition of a certain scientific concept. Brechet et al. [3] had over 250 French children draw their brains to understand how the “black box” of the brain works. In terms of emotions and interpersonal relationships, campus bullying is a topic of great concern to psychologists, so drawing has also been used in related research. For example, Marengo et al. [4] analyzed more than 600 drawings of Italian children to understand this issue.
However, there are still some fundamental issues that have not been overcome in the current psychological analysis of children’s drawings, which mainly focus on methodological perspectives. This is mainly reflected in two aspects: firstly, the issue of ecological validity. As drawing is a non-verbal representation, it must be digitized through effective encoding in order to be analyzed using statistical methods. Therefore, encoding method is currently the mainstream way to study children’s drawing. For example, Tolsberg et al. [5] used coding to understand the longitudinal development of math word problems among Estonian elementary school students. The coding rule for that study is as follows: those without drawings are coded as 9, those depicted as pictorial are coded as 1, and those depicted as schematic are coded as 2. The main difference between 1 and 2 is whether the relationship between the things is revealed. Therefore, how to develop coding rules is a crucial issue. This issue is particularly prominent in learning science, as emotions and interpersonal relationships have a large number of scales or tools (such as bullying problems [6]), and some drawing studies on this issue can use these general tools [4]. However, many researchers in learning science area had to adopt self-developed coding rules. In a psychology journal, there were a total of 12 original studies on the topic of children’s drawing research, of which only four studies did not involve self-developed coding at all (only 33.3%) [7].
The high proportion of self-developed coding naturally raises a question: how can these highly task-dependent coding rules be transferred and generalized to other drawing studies? In other words, different studies have different tasks, so how should other drawing studies refer to the results of these codes? Obviously, existing drawing research lacks a unified norm, which allows students to draw a certain field or object relatively freely under the condition of no tasks or fewer tasks, in order to fully expose their drawing preferences for that object in the naturally state. If this norm exists, different drawing tasks can be compared with it, then there is a common reference frame for drawing research on the same theme.
The second issue is more obvious: the understanding of drawing is one of the most prominent areas of individual heterogeneity, and different people’s perspectives on the same drawing may be completely different [8]. Therefore, for studies do not utilize machines, consistency testing is an essential step—the same drawing must be identified by at least two researchers, and the results of these two identifications must pass consistency testing (such as Cohen Kappa coefficient [3, 5]). On the contrary, if machine learning models are used to identify drawings, this process is likely to be omitted. Because the same model is based on the same algorithm, its generation results should be more consistent than those of researchers.
Based on the above two issues, this study aims to construct a norm of drawings for children’s understanding of scientific themes and to use the Large Language Model (LLM) to understand and identify children’s drawings, attempting to provide a reference frame for future research on children’s conceptual understanding of drawing. This study focuses on two questions: (i) Do children’s drawings have a consistent representation for the same drawing theme (such as whether the concept of solar eclipse is mostly expressed through light path diagrams)? (ii) What factors are related to the consistent representations (if it exists)?
Select a total of 1473 students from two primary schools in Beijing. Among them, there are 74 students in grade 4, and the rest are in grades 5 and 6. The male to female ratio is 1.01:1. The textbooks involved two versions [9, 10].
We excluded drawings that were unclear, incomplete, or clearly did not belong to the prescribed content, as well as drawings with inconsistent recognition results by LLM multiple times (see section “LLM Identification of drawings” of this article) and finally included a total of 1420 drawings in the sample.
All children’s personal information is strictly confidential, and the drawings were sealed after being photographed to ensure that personal information of students were not leaked. The students’ names/IDs were also removed. The method of removing is as follows: firstly, open a children’s drawing using Photoshop. Secondly, select the “Clone Stamp Tool” function in the left tab and adjust the size of the icon to easily cover the name/ID. Thirdly, hold down the Alt key while selecting the area without any graphic content around it. The main purpose of this step is to let the software know which area of the image to use to overlay the image containing names/IDs. Fourth, release the Alt key and draw the area where the name/ID is located. In this way, the software can use the selected area to overlay the image of the area where the name/ID is located, achieving the goal of overlaying the name/ID. And after this operation, the screen appears natural and will not attract the attention of LLM due to missing parts of the screen, causing it to misjudge. Informed consent from the participants’ guardian was obtained. This research was approved by the Research Ethical Review Committee of Beijing Doers Educational Consulting Co., Ltd on 13 March 2024 (ECS-008).
Firstly, all children draw in class, i.e. they draw whatever they just learned (following the order of textbooks), without deliberately setting contents or tasks. Secondly, to ensure the consistency and interpretability of the data, all participating teachers in this study are required to strictly follow the textbook and strictly prohibit free design teaching scaffolds and guidance to prevent individual differences among teachers from interfering with the results. Thirdly, the prompt was: “We have just learned the knowledge of …, please use drawing to represent it.” Other than that, no hints will be given to prevent any suggestion or misleading to students. Fourth, ensure that all students can complete the drawing.
Firstly, number and take photos of all the drawings. Secondly, preprocessing the photos includes two parts: (i) removing students’ personal information, such as names and student IDs, and (ii) removing the theme/concept names (some students have the habit of writing the names of the themes/concepts on their drawings) to prevent prompting LLM. The solution is to use the “Clone Stamp Tool” function of Photoshop software (v.2018) to cover these names with surrounding images.
Firstly, input the photos into LLM for identification. The prompt is “What is the specific content of this drawing? How did you see it from the picture?” and copy all the results and reasons generated by LLM into an Excel file for analysis.
In addition, to ensure the stability of LLM recognition, all drawings are independently recognized twice by LLM. Drawings with significantly different recognition results (such as the first recognition as a solar eclipse and the second recognition as a lunar phase) will undergo a third independent identification, and those with two occurrences of the three results will be considered as the result. If three different results occur, the drawing will be removed. The so-called “independence” refers to deleting the current page after one recognition is completed and re-recognizing it. Since the LLM used does not capture conversations with front-end users and place them in its pre training set, the above operation method can ensure the independence of the two recognitions.
This study used ChatGLM-4.0 to identify drawings. The reason for adopting this LLM is that its image recognition models mainly use convolutional neural networks (CNN) and recurrent neural networks (RNN) (derived from the self-introduction of this LLM), and these two models have been proven to be efficient and reliable in image recognition (because the two models are so famous and there are too numerous related studies, so this article will not list the literatures here). And there have been some studies published using this LLM for image recognition [11, 12], further proving its reliability in image recognition. And its language generation method adopts a model based on the transformer algorithm, which has high efficiency and reliability in language generation. More importantly, the basic version of this LLM is open to individual users for free, making it more suitable for numerous primary school teachers to identify children’s drawings.
To verify that LLM’s image recognition reasonings can indeed explain children’s drawings representations, we counted the number of objects appearing in the drawings in all LLM image recognition reasoning (as long as one object appears in the drawings in these sentences, it is considered). The results showed that 77.4% (weighted average) of the reasoning statements for image recognition contained at least one object appearing in the drawings. This indicates that LLM is indeed describing the specific content of the drawings, rather than fabricating reasons. At the same time, in order to prevent LLM from generating image recognition reasons by reading text on the drawings, we randomly selected 16 drawings without any prompts (2 drawings for each theme/concept, and all drawings of the “solar system” concept have prompts, so this concept cannot be selected). The results showed that LLM described at least one object in all the 16 drawings.
This study used the semi-supervised Word2vec algorithm to generate word vectors and then used the cosine similarity algorithm to calculate semantic similarity values.
The Word2vec is a deep learning algorithm for word embedding proposed by the Tomas Mikolov team at Google. Traditional spatial distance–based algorithms (such as Euclidean distance or Manhattan distance) in handling sentences with the same semantics but different representations usually have poor performance [13]. In contrast, the Word2vec algorithm creatively proposes a negative sampling method, which calculates the conditional probability of the sampled sample coming from the corpus through context word frequency statistics and then determines its polarity [14]. This method can better identify similar semantic sentences with different representations.
Cosine similarity is a widely used algorithm for calculating semantic similarity. Its main principle is to model the text as a term vector and calculate the cosine value between the term vectors to determine the degree of similarity between two texts [15]. The cosine similarity algorithm has significant advantages in semantic similarity calculation, with evidence showing that it has higher resolution than Jacobian similarity [16].
By incorporating supervision into unsupervised algorithms, it has been widely applied in NLP semantic analysis and comparative research on cross-class text learning between machines and humans, demonstrating its advantages [17].
Firstly, we collated all the statements generated by LLM for image recognition, match the statements with the image numbers one by one, and outputted the results as an Excel file. Then, used the JIEBA tokenizer to segment the statements in this Excel file to words (removed all punctuation marks). Then, we trained a semi-supervised Word2vec algorithm (dimension 100, context window size 5, negative sampling 5) to embed word vectors into the segmentation results and then used the word vector embedding results to calculate the average word vector of each sentence. Finally, the cosine similarity algorithm was used to calculate the semantic similarity between all pairs of sentences, thereby forming a similarity matrix.
In addition, although the theoretical range of semantic similarity calculated by the cosine similarity algorithm should be [−1,1], all semantic similarity results in this article are greater than 0. Therefore, for the sake of simplicity in calculation, all calculations regarding the concentration of semantic similarity in the following text were defaulted the value range of [0,1] instead of [−1,1].
Due to Word2vec being an unsupervised algorithm, supervised learning is clearly superior in most cases in NLP processing. Therefore, we introduce keywords to transform the above algorithm into a semi-supervised learning method, thereby increasing the accuracy of word vector embedding.
The specific method was to first use the JIEBA tokenizer to segment the text to be analyzed and remove all punctuation marks. Second, the TF-IDF algorithm to search for keywords in the text was used and assigned weights to these keywords. The number of keywords of each text was set at 10, because after repeated experiments, when the number exceeds 10 (such as 15), there will be a large number of high-frequency function words (such as “this” and “yes”) in the keywords. Third, semi-supervision by adding Python statements was implemented. Finally, the generated word vectors into an Excel file to calculate the average word vector for each sentence in the original text was embedded.
Used a heat map to represent the similarity values of the semantic similarity matrix for each theme/concept and used a histogram to represent the specific distribution of similarity in [0,1].
Specifically, as this article mainly focuses on the concentration of semantic similarity in LLM image recognition reasons. But the traditional quartiles from Q1 to Q3 can only reflect the concentration of 50% of the data. In order to intuitively reflect the degree of concentration of more data, this article constructs a new indicator based on the construction method of quartiles: all semantic similarity values of a concept are arranged in ascending order and evenly divided into eight parts. In this way, from the Q1 (equivalent to the Q1 of quartiles) to Q7 (equivalent to the Q3 of quartiles) encompass three-quarters (75%) of all data. So, calculating the concentration from Q1 to Q7 of the new indicator is equivalent to knowing how concentrated 75% of the total data is (locally referred to as the “75% Region”). The smaller the value, the higher the numerical concentration, which is used to examine whether the theme/concept has a consistent representation.
Finally, we used Kendall rank correlation coefficient (Kendall’s τ) to calculate the effects of three factors: Sample Size, Abstract Degree of Themes/Concepts, and Reasoning Degree of Themes/Concepts on the Accuracy, Semantic Similarity, and 75% Region of children’s drawing representations.
There are two reasons for using this indicator (Kendall’s τ): firstly, this article is only an observational study, not a control study, so exploring correlation is more suitable than establish the causal relationship [18]. Secondly, the related factor exploration in this article aims to understand the impact of a certain factor on nine concepts, which means we only have a sample size of 9. The commonly used Pearson correlation coefficient is an indicator that strictly depends on sample size, but due to the limitations of researchers, we do not have the ability to explore more concepts of children’s drawings. The Kendall’s τ is more suitable for dealing with low sample size correlation problems, so we chose this indicator to explore the relevant factors.
At the same time, we also examined the reproduction rate of children’s experiments/phenomena taught in class (mainly for abstract themes/concepts) through the method of word frequency. Because we suspect that students may have difficulty grasping the connotations of abstract concepts, they may use experiments/phenomena taught in class to represent these concepts in drawings.
The total number of samples is 1420, covering nine scientific themes or concepts: Circuits, Solar eclipses, Boiling, Solar system, Increasing the carrying capacity of a boat, Life history of a plant, Buoyancy, Electromagnetics, and Physical and chemical changes. The average recognition accuracy of LLM is 46.7%, and the median semantic similarity is 0.9885. In addition, the mean of the “75% Region” is 0.0035 (i.e. occupying 0.35% of [0,1]). The data is detailed in Table 1.
Overall situation of the samples (where 75% refers to the 75% Region, with smaller values being more concentrated).
From the overall situation, it can be seen that LLM has a low recognition accuracy for image drawing content, but the semantic similarity concentration is high. This seems to imply that there was a consistency preference in the representation of most of the drawings, but there is a possibility of “collective bias” in this preference (i.e. those children’s drawing representations were inaccurate, but the way these erroneous representations were consistent).
The main learning content (experiment) of this concept is to light up a bulb to let students know that the circuit must be closed in order to work (i.e. form a closed loop).
The average accuracy of LLM image recognition is 84.3%, which is the highest among all nine themes/concepts, indicating that most students can grasp the correct representation method of this concept.
The median value of semantic similarity is 0.9998, and 75% Region is concentrated within 0.02% of [0,1], indicating that the vast majority of students have a high degree of consistency in the representation of the form of circuits. Considering the high accuracy, it indicates that the majority of students’ drawings are correct and consistent. For specific details, please refer to Figs 1 and 2, where Fig. 1 is a numerical analysis and Fig. 2 is the “representative drawings.” The so-called “representative drawings” refer to a pair of drawings with semantic similarity located in the median of the 75% Region. As they are in the median, they should be the most representative of all drawings, and the same applies in the following.
Distribution diagram (left) and heatmap (right) of semantic similarity of the concept of “Circuit.” The heatmap on the right specifically refers to the semantic similarity data distribution of the ≥0.9 part (the same below). The horizontal axis of the distribution chart represents the distribution of semantic similarity between concepts [0,1], and the vertical axis “Count” represents the total amount of semantic similarity data contained in the concept similarity matrix. The horizontal and vertical axes of the heatmap represent the sample size of the concept (since each image corresponds to a recognition reason, it also represents the number of recognition reasons), so the heatmap represents the semantic similarity between any two recognition reasons of the concept. In terms of color, the semantic similarity between all [0.9, 1] is divided into five levels based on the distribution of semantic similarity of the concept, represented by blue, green, yellow, orange, and red colors from low to high (excluding [0.9998, 1] and [0.9999, 1])
Representative drawings of the concept of “Circuit”
The main content of this concept is to understand the phenomenon and causes of solar eclipses, but it does not require representation using light paths.
The average accuracy of LLM image recognition is 44%, which is basically at the average level of nine themes/concepts.
The median value of semantic similarity is 0.9999, and 75% Region is concentrated within the 0.001% range of [0,1], indicating that the vast majority of students have a high degree of consistency in the representation of this concept (Figs 3 and 4). However, it is worth noting that the accuracy of LLM image recognition is not high, indicating the possibility of “collective bias” in students’ representation. We found a possible reason for the high consistency rate but low accuracy rate when rechecking children’s drawings: some students did not annotate any explanation (only drew three circles representing the sun, earth, and moon, without any signs), and the drawings did not look like them, which led LLM to mistake them for bicycle chains or other unrelated objects.
Distribution diagram (left) and heatmap (right) of semantic similarity of the concept of “Solar eclipse”
Representative drawings of the concept of “Solar eclipse”
The main content (experiment) of this concept is to boil water through alcohol lamp. The textbook requires students to know the phenomenon of water boiling and the temperature changes during this process (i.e. it keeps rising until about 100°C and no longer changes).
The average accuracy of LLM’s image recognition is 0, ranking at the bottom of the nine experiments, indicating that all students’ representations misled LLM.
The median value of semantic similarity is 0.967, and 75% Region is concentrated within the 1.49% range of [0,1]. Although the 75% Region belongs to the second to last of the nine concepts, the concentration is still very high, indicating that the vast majority of students have a high degree of consistency in the representation of boiling forms (Fig. 5). However, the image recognition results of LLM are completely unrelated to this concept, indicating that almost all student representations have problems. This result is very surprising, as can be seen from the representative drawings in Fig. 6 where the problem lies: the vast majority of students’ drawings focus on the experimental process and equipment, with almost no description of the changes in water during this process. This experiment does indeed require the use of this experimental equipment, but these devices are not what this class is going to learn, which means that students’ focus is entirely on the experimental activity itself—rather than what the experiment is going to explain (changes in water), which is truly unexpecting.
Distribution diagram (left) and heatmap (right) of semantic similarity of the concept of “Boiling”
Representative drawings of the concept of “boiling”
It should be noted that we modified a keyword while tuning the model training related to this concept. Due to the significant difference between the results provided by the TF-IDF and our feeling, we conducted a retrospective analysis of all processes and found that an important word, “chemistry,” was not recognized as a keyword by TF-IDF. However, this word appeared very frequently in the reasons given by LLM (about 49% of reason sentences mentioned this word). Since the TF-IDF algorithm is a statistical method based on word frequency, it cannot truly understand the meaning of the text, and the word “chemistry” appears only once in each sentence, far less frequently than “this,” so it is not surprising that it is excluded by algorithm. Based on this, we selected the function word with the lowest weight from the 10 keywords provided by TF-IDF, replaced it with “chemistry,” modified the corresponding word weight, and attempted to retrain. The results were very close to our intuitive feeling.
The main content of this concept is to understand the solar system, especially the Sun and the eight major planets and their relative positions, but it does not require students learn the motion state and trajectory.
The average accuracy of LLM image recognition is 0.65, which is higher than the median value, indicating that most students can correctly represent this concept.
The median value of semantic similarity is 0.9999, and 75% Region is concentrated within the 0.001% range of [0,1]. Similar to the concept “Circuits,” high accuracy and consistency in representation indicate that children have a correct and consistent understanding of this concept (Figs 7 and 8).
Distribution diagram (left) and heatmap (right) of semantic similarity of the concept of “Solar system”
Representative drawings of the concept of “Solar system”
The main content of this theme in textbook is understanding the relationship between buoyancy and the volume of displacement by changing the volume and load capacity of the boat. But the presentation of this lesson in the textbook is quite unique: first, boats with different bottom areas were made using aluminum foil of the same area, and then small iron hoops were added to different boats to increase their weight and test their load-bearing capacity. Ultimately, this guides students to identify factors associated with buoyancy. That is to say, students cannot directly adjust variables, but need to first create a “boat.”
The average accuracy of LLM’s image recognition is 0.033, second only to “boiling” and ranking second to last among the nine concepts, indicating that the vast majority of students’ representations have misled LLM.
The median value of semantic similarity is 0.987, and 75% Region is concentrated within the 1.29% range of [0,1]. Overall, the result of this concept is very similar to “Boiling”: with low accuracy but high semantic concentration, indicating that children’s drawing misled LLM in the same representation way (Fig. 9).
Distribution diagram (left) and heatmap (right) of semantic similarity of the concept of “Increasing the carrying capacity of the boat”
However, the reason for this result differs greatly from the concept of “Boiling,” which is due to students’ excessive focus on the experimental process rather than the experimental purpose, and the problems with this theme are more caused by the textbook. As mentioned above, the original goal of this lesson was to understand the relationship between buoyancy and the volume of displacement. However, in terms of presentation, the textbook first requires students to use aluminum foil to make a boat and then complete the task of adjusting the weight. That is to say, there is a “plot transition” in this lesson, and from the representative drawings below, it can be seen that the vast majority of children were stuck at this transition point—they were most impressed with how to make a boat, rather than adjusting variables. Therefore, the most frequently depicted object in the drawing was a square aluminum foil paper with creases left by the boat making around it (Fig. 10). This directly led to errors in the overall direction of LLM: from the perspective of word frequency statistics, 59.6% of LLM’s image recognition reasons showed that it was a square box–like object that had nothing to do with the boat.
Representative drawings of the concept of “Increasing the carrying capacity of a boat” (the four circular objects in the drawing on the right were small iron hoops used to increase the weight)
The main content of this theme in the textbook is the life history of a plant, which refers to the entire process of the plant from seeds to flowering and fruiting. In the previous classes, students have already gone through tasks such as cultivating and caring for plants and recorded the growth and changes of the plants. Therefore, the main purpose of this lesson is to establish a comprehensive impression of the entire process of the plant development and changes through previous records.
The average accuracy of LLM image recognition is 0.712, ranking second among the six concepts, indicating that most students can correctly represent this theme.
The median value of semantic similarity is 0.9999, and 75% Region is concentrated within the 0.001% range of [0,1]. Figures 11 and 12 explain that children have a correct and consistent understanding of this concept.
Distribution diagram (left) and heatmap (right) of semantic similarity of the concept of “Life history of a plant”
Representative drawings of the concept of “Life history of a plant”
This theme is very similar to the goal of “increasing the carrying capacity of a boat,” which is to control the sinking or floating of the boat by changing its weight and shape. These two themes come from two different versions of textbooks, so there is some overlap in content. But unlike “increasing the carrying capacity of a boat,” this lesson did not ask students to make a boat, but instead posed the core question at the beginning: how to keep a small boat floating on the water sink? Therefore, when learning this theme, the entire class was focused on exploring how to adjust variables and control the buoyancy of the boat, not to make a boat. So, as will be seen in the following text, there is a significant difference between the results than “Increasing the carrying capacity of the boat.”
The average accuracy of LLM’s image recognition is 0.5, ranking six among nine concepts, indicating that a considerable number of students’ drawings misled LLM, but it is significantly better than “Increasing the carrying capacity of the boat.”
The median value of semantic similarity is 0.944, with 75% Region concentrated within [0,1] of 3.38%, ranking last among all nine concepts (Fig. 13). After rechecking all the drawings, we found the main reason for the divergence of semantic similarity: the textbook of this lesson did not strictly define what is a “boat,” so as shown in the representative drawings below (Fig. 14), some students used foam plastic and leather balls. We also found that materials such as plastic balls, wood blocks, iron boxes, well-made submarine models and warship models were used in some drawings. It is precisely because the textbook does not impose too many restrictions that students exhibited rich imagination, which also leads to the diversity of LLM’s descriptions of the reasons for image recognition.
Distribution diagram (left) and heatmap (right) of semantic similarity of the theme of “Buoyancy”
Representative drawings of the theme of “Buoyancy”
This concept in textbook is mainly reproduced through the experiment of electromagnetic phenomenon discovered by Danish scientist Hans C. Oersted in 1820. And enhance electromagnetic phenomena through wire winding to make coils, to explore related factors that affect the strength of this phenomenon.4
The average accuracy of LLM image recognition is 0.514, ranking fourth among nine concepts.
The median value of semantic similarity is 0.9995, and 75% Region is concentrated within the 0.11% range of [0,1], indicating that students have strong consistency in their representation of this concept (Figs 15 and 16).
Distribution diagram (left) and heatmap (right) of semantic similarity of the concept of “Electromagnetics”
Representative drawings of the concept of “Electromagnetics”
The main content of this theme is to explore various changes in the kitchen, and to understand the difference between “physical changes” and “chemical changes” based on whether new substances are produced. This theme mainly corresponds to the “Next Generation Science Education Standards” (NGSS): “When two or more different substances are mixed, a new substance with different properties may be formed” (5-PS1-4) [19].
The average accuracy of LLM image recognition is 0.513, ranking fifth among the nine concepts, i.e. in the middle.
The median value of semantic similarity is greater than 0.9999, and 75% Region is concentrated within almost 0% range of [0,1], indicating that students have a huge strong consistency in their representation of this theme (Fig. 17). It should be noted that, as shown in the representative drawings below, children exhibit two completely different tendencies toward drawing this theme: one is to express it through graphics, and the other is to express it through mind maps (Fig. 18). However, from the perspective of semantic similarity, the difference between these two forms of expression does not affect the understanding of semantics by the word2vec algorithm, because even when using graphics to represent this theme, a considerable number of students have added prompts in the drawings. The detailed information will be elaborated in the “Discussion” section of this article.
Distribution diagram (left) and heatmap (right) of semantic similarity of the concept of “Physical and chemical changes”
Representative drawings of the theme of “Physical and chemical changes”
Due to the overall accuracy rate being only 46.7%, there is a possibility that such a low accuracy may be due to the low image recognition ability of the LLM we have chosen? For this purpose, we conducted two additional experiments: validation from human and ChatGPT. We recruited a total of 46 adult participants and identified 225 drawings (accounting for 15.8% of the original 1420 drawings). In addition, a total of 284 drawings were identified using ChatGPT-4 (20% of the original 1420 drawings). The selection of drawings adopts a pseudo-random method, i.e. first use the RANDBEATWEEN command in Excel to generate a column of pseudo-random numbers, which correspond to the serial numbers of children’s drawings, and then we select the drawings for recognition and verification based on the pseudo-numbers.
Since we used a sampling method, we first tested whether a sampling rate of 15.8% and 20% could represent the overall results. We retrieved the accuracy data of ChatGLM for recognizing 225, 284, and all 1420 children’s drawings and verified the difference between the two samplings and population using chi-square test. The result of “225” is χ2(1)=1.759, P = .185. The result of “284” is χ2(1)=0.992, P = .319. So, there are insufficient evidence of significant differences, indicating that a sampling rate of 15.8% and 20% can basically explain the overall situation.
The results of the accuracy comparison are shown in Table 2 and Fig. 19. It can be seen that the performance of humans and the two LLMs in children’s drawing recognition accuracy is very close. To further validate this intuitive feeling, we conducted a one-way ANOVA analysis on the four columns of data in Table 2. The results showed that, F(3, 32) = 0.217, P (3, 32) = .884. There is almost no significant difference.
The performance of humans and LLM in identifying nine concepts. The horizontal axis 1–9 in the left figure corresponds to nine concepts, respectively
Comparison of human, ChatGPT, and ChatGLM.
Among them, “ChatGLM total” represents the recognition accuracy of each concept in the original 1420 children’s drawings by the LLM, while “ChatGLM” represents the recognition accuracy of the 284 children’s drawings in the additional experiment corresponding to the LLM.
Therefore, overall, we believe that the choice of LLM does not affect the conclusion of this article.
If the accuracy of human and LLM image recognition is similar, are their reasons for image recognition also very similar? Therefore, we recruited human participants to recognize more children’s drawings, in order to have the opportunity to compare the diversity of language generated by humans and LLM. A total of 46 adult participants were recruited this time, and 225 children’s drawings were identified. As LLM participants, ChatGPT-4.0 also recognized the same drawings.

Figure 20 compares the quartiles of semantic similarity between humans and GPT, while Fig. 21 compares the distributions. It can be clearly seen that the semantic similarity of human language is significantly lower than that of machines.
The quartiles of semantic similarity between humans and GPT
The semantic similarity distributions between humans and GPT
However, what could be the reason for this difference? We have made two speculations: one possibility is that human language generation ability is stronger than LLM, i.e. humans can generate and organize language using richer expressions. Another possibility is that human language contains richer meanings, and LLM may just be honestly “ discuss it literally.” Of course, it is also possible to have both.
To verify these two possibilities, we first conducted word frequency statistics on both human and GPT generated languages to verify whether their word frequencies conform to the Zipf distribution. The results are shown in Table 3 and Fig. 22 (left). It can be seen that there are indeed some differences between the two, but they are not obvious. The independent t-test confirmed this point (F(16) =0.001, P (16) = .223). It can be seen that there is not much difference between human language and GPT.
The results about Zipf (left) and LDA (right) between humans and GPT
The Zipf values between humans and GPT.
To verify the second hypothesis, we used LDA algorithm to explore the differences between human and GPT languages in terms of latent topics, as latent topics can well represent the diversity of text meanings.
To this end, we first used LDA algorithm to mine the potential themes of human and GPT generated text of each concept and listed the top 10 main themes. Then, using the method of information entropy, the entropy values of these topics were calculated to represent the diversity of topics in different texts. In order to ensure the stability of the data and comparability of the results, we fixed the parameters of LDA in filtering the text as follows: no_before = 3, no_before = 0.8, and keep_n = 100 000. The comparison of information entropy between them is given in Table 4 and Fig. 22 (right).
The information entropy of LDA between humans and GPT.
It can be seen that the entropy value of human language is significantly higher than GPT (average entropy HGPT = 1.459, Hhum = 2.188, t-test; P<.001), confirming the second hypothesis that the diversity of potential topics in human language is significantly higher than that of machines.
By reviewing the reasons for human and machine drawing recognition, we found that there are at least three aspects that may confirm the above results: firstly, the diversity of human participants composition. About one-third of all 46 participants were science teachers, who not only had significant differences in their language compared to other participants, but also had a phenomenon of judging children’s drawings. Such as “The diagram depicts the major celestial bodies in the solar system, indicating that students have a relatively rich understanding of the solar system,” which is impossible to see in the generative language of LLM. The second is the semantic diversity caused by rich experiences. There is a certain degree of similarity between “Life History of a Plant” and another lesson “Seed Germination,” so a considerable number of human participants confuse these two lessons, while LLM does not have confusion due to the lack of similar experiences. The third reason is other factors. Such as human general knowledge is richer than LLM (one human participant misunderstood a child’s drawing as one of the Chinese mahjong tiles, but LLM may don’t know what is the mahjong and naturally will not think in that direction), or homophone lead to misleading tokenizers and algorithms.
The above results further demonstrate the value of using machines for this study: the results of human participants are greatly influenced by individual heterogeneity, but machines almost do not have this problem. Although the results of machines may not be superior to those of humans, their results have strong data stability, are easier to reproduce, are more friendly to statistical analysis, and are more suitable for mining patterns from them.
As mentioned above, this article uses Kendall rank correlation coefficient (Kendall’s τ) to analyze the factors that affect drawing’s representations.
Is there a relationship between the above results and the total sample size? We separately calculated the correlation coefficients between “sample size” and three factors (“accuracy,” “median value of semantic similarity,” and “75% Region”), and the results showed that the correlation coefficients of the three factors were 0.611, 0.609, and −0.493, respectively (the three bar charts above Fig. 24). It can be seen that sample size has a strong correlation with accuracy and median value of semantic similarity and has a moderate correlation with semantic similarity concentration, and this moderate correlation is not significant P = .07).
Based on J. Piaget’s theory of developmental stages in children, the abstract thinking ability of elementary school students has not yet been fully developed [20]. Therefore, we speculate whether the above results are related to the degree of abstraction of concepts? That is to say, the higher the degree of abstraction, the lower the accuracy and the lower the semantic similarity? However, how to define the degree of abstraction of concepts? Children and adults have different understandings of abstraction. Some adults believe that concrete concepts (such as mathematical laws of force or motion) are abstract and difficult to grasp in the eyes of children because they cannot be directly seen [21].
Therefore, we have designed and implemented a new experiment here. We have recruited 10 new raters to complete a 5-point Likert scale (very abstract, abstract, neutral, concrete, very concrete). These participants all have over 7 years of teaching experience, and 4 of them hold the title of “Senior Teacher” of science education. The overall results are shown in Table 5, assign the 1 to 5 points from “very abstract” to “very concrete,” respectively.
The result of five-point Likert scale.

Figure 23 is a bar chart of the average ratings of nine concepts, with the red horizontal line representing the average level of ratings for all concepts.
The result of scores about each concept
We used the within the Intraclass Correlation Coefficient (ICC) instead of Cohen’s Kappa test for consistency testing. The results showed that the Cronbach’s alpha α = 0.618, ICC(3,1) = 0.139, ICC(3,9) = 0.618. The overall behavior of the 10 raters shows a moderate to strong consistency.
According to the results in Table 5, we calculated Kendall’s τ between the Likert mean score of each concept and its “75%,” median, and accuracy. The results were −0.03 (P = .914), −0.029 (P = .916), and 0.057 (P = .833), respectively. This result suggests that the level of abstraction of concepts may not be significantly correlated with all these factors.
Another factor we are concerned about is whether students will express abstract themes/concepts by reproducing contents that have appeared in class through drawings? Due to the elusiveness of these themes/concepts for children, textbooks often present them through representative examples, such as express the “Life history of a plant” through Impatiens. So, will students represent these difficult things by reproducing the examples that have appeared in the classroom? We used the examples that appeared in the textbook as the basis and calculated the proportion of these example words in the LLM’s recognition reasons through word frequency statistics and calculated the total proportion using weighted average (weighted by sample size). The results are as follows (Table 6):
Reproduction rate of samples encountered in the classroom.
It is truly difficult for us to identify whether the plants in drawings were Impatiens or not (most of the drawings do not provide the names of the plants), so we did not calculate the data for this theme.
Due to the large number of examples appearing in this lesson, it is not required that all examples be drawn. As long as one example appears in the drawings, it will be counted.
The weighted average reproduction rate is 76.19%, which means that at least three-quarters of the students have reproduced the classroom content in the drawings more or less to represent these difficult things. Considering that LLM’s recognition accuracy is not 100%, this proportion is likely to be even higher.
This is what we discovered after carefully examining the drawings: we found that the concept of “boiling” originally required students to understand the process of water to steam change, but most of the students drew the experimental activities (i.e. the experimental apparatus). This experiment is an indispensable process for heating water, but not the purpose of the lesson. However, it was used by students as a representation of the phenomenon of water boiling. In other words, there is a significant deviation between the students’ focus and the purpose in the textbook.
Therefore, we developed a coding rule based on the role of experiments in lessons and used it as an independent variable to conduct correlation analysis on the above three factors. The coding rules are as follows: those themes without experiments or experimental results that were equivalent to the teaching purposes will be coded 1. The experimental results were not the teaching purposes, but only the indispensable process of teaching purposes will be coded 2 (simple drawing and hands activities are not considered experiments). The coding result is as follows (Table 7):
Coding and reasons for nine concepts about the relationship between experimental results and teaching purpose.
Due to the fact that the above coding rule involves a deep understanding of textbook content, we invited two senior science teachers (both of whom have senior professional titles) to review the above rules and received their unanimous agreement.
The correlation coefficients are −0.624, −0.488, and 0.465 (the three bar charts below Fig. 24), indicating that the consistency between experimental results and teaching purposes can explain the accuracy very well. Specifically, it can effectively solve the problem of “collective bias” in students’ drawings: students are more concerned with the experiment itself, but if the experiment deviates from the teaching purposes, students are likely to choose to remember the experiment rather than the real purposes. Meanwhile, there is a moderate correlation with the other two items (median value of semantic similarity value and 75% Region).
The Kendall rank correlation coefficient results for each factor, where “samp” represents the factor of sample size, “abst” represents the factor of abstract degrees, “focu” represents the factor of focus points, “75%” represents 75% Region, “medi” represents median, and “corr” represents accuracy; “samp-medi” represents the correlation coefficient between the sample size and the median value (other similar); Color represents moderate or above correlation (tau > 0.4), and bold black boxes represent the correlation is significant (P < .05)., where italicized numbers represents the P-value of the correlation coefficient
This article outlines the representations of nine scientific concepts in children’s eyes through 1420 children’s scientific drawings and explores whether there is a consistent representation in children’s scientific drawings through LLM image recognition and semantic similarity analysis. The results show that the representation of most drawings has consistency, manifested as most semantic similarity > 0.8. At the same time, it was found that the consistency of the representation is independent of the accuracy (of LLM’s recognition), indicating the existence of consistency bias. In the subsequent exploration of influencing factors, we used Kendall rank correlation coefficient to investigate the effects of “sample size,” “abstract degree,” and “focus points” on drawings and used word frequency statistics to explore whether children represented abstract themes/concepts by reproducing what was taught in class. It was found that accuracy (of LLM’s recognition) is the most sensitive indicator, and data such as sample size and semantic similarity are related to it. The consistency between classroom experiments and teaching purpose is also an important factor, and many students focus more on the experiments themselves rather than what they explain. In addition, most children tend to use examples they have seen in class to represent more abstract themes/concepts, indicating that they may need concrete examples to understand abstract things. In summary, this article provides a novel approach to understanding children’s drawings, which utilizes LLM to convert drawing information into more accurate linguistic information, allowing for in-depth research on children’s conceptual learning using linguistic tools.
The first thing that catches attention in the above results is that LLM’s image recognition accuracy is not high (46.7%). The additional tests on humans and ChatGPT mentioned earlier have shown that this result is almost unrelated to the selection of LLM. So, how can this phenomenon be explained? We believe that in addition to the factor of children’s low drawing ability (which is actually a very important factor), there is another factor that cannot be ignored: many children’s drawings that demonstrate conceptual understanding shown the thinking process of children, rather than the results or products of thinking.
As shown in Fig. 25, a square box is drawn in the middle of the image, with four circular objects surrounding the box. At first glance, it is difficult for anyone to associate this drawing with a small boat. The background of this drawing is derived from the lesson “Increasing the carrying capacity of a boat.” According to the textbook design, the students’ activities in this lesson mainly include two items: first, folding a piece of aluminum foil into a square box without a lid, which can be used as an abstract version of a small boat. The second item is to use some circular iron rings (i.e. the four circular objects in the picture) to adjust the carrying capacity of the ship, ensuring that the small boat will not sink. The main intention of this lesson is to help students discover the relationship between the bottom area and height of a ship by adjusting the number of iron rings (because the total area of aluminum foil has already been fixed, changing the bottom area will inevitably change the height of the ship). However, it can be clearly seen from the picture that the students’ thinking seems to have completely paused in the “making a small boat” item, with only the four circles leaving a little trace of “adjusting the load capacity.” That is to say, this drawing is an unfinished product of thought, reflecting the process of students’ learning and thinking.
It’s hard to imagine that the student is drawing a small boat in this picture
However, LLM will definitely recognize the drawing as a mature and complete thinking result by default. And unfortunately, the images are the only source of information for LLM to recognize the drawings. And this deviation will naturally lead to a lower accuracy rate in image recognition. Moreover, from the above results, it can be seen that the accuracy of human image recognition is not much higher than that of LLM, indicating that human participants also assume that a drawing is a complete thinking product when they receive it.
However, looking back at the entire result, we found a rather strange thing: some themes/concepts (solar eclipse, buoyancy, electromagnetics, and physical and chemical changes) have an accuracy rate close to 50%, but their semantics are very close to each other (median > 0.99). In this study, semantic similarity is used to represent the consistency of students’ drawing representations. A high accuracy rate and high semantic similarity indicate that students are able to correctly and consistently represent the concept with drawings. A low accuracy rate but high semantic similarity indicates that students may have a problem of “consistency bias.” So, how can we explain why the accuracy is close to 50% but the semantic similarity is high? The accuracy rate approaching 50% indicates that there should be significant differences in students’ drawings (i.e. the number of correct and incorrect drawings is almost equal), why is the semantic similarity still high? This seems somewhat abnormal.
We carefully reviewed these drawings and found a possible “suspect”: some drawings that LLM deemed incorrect actually used representations that were very similar to those of the correct drawings, but lacked some key prompts, thus misleading LLM. As shown in Fig. 26, both the left and right images depict the transformation of water into ice and the oxidation of apples after being bitten, but the difference is that the left image clearly indicates “physical changes” or “chemical changes” below each change, while the right image does not indicate these words. So, the left image was correctly recognized by LLM, but the right image was not.
Both left and right images depict the process of water turning into ice and the oxidation of apples
Another issue we are concerned about is how children understand abstract concepts, because obviously, abstract things are more difficult for people to learn concepts than concrete concepts, especially for children. For example, Elika Bergelson et al. [22] found that infants and young children have a significantly later understanding of abstract vocabulary than concrete vocabulary, but learning abstract things is an important task in education.
Previous research has focused on the following directions: firstly, utilizing language itself. For example, Gabriella Vigliocco et al. [23] proposed after reviewing previous research that children can use context to assist in understanding more abstract concepts. In fact, this is also the basic principle based on which mainstream NLP algorithm models are currently based [24]. In addition, Yanchao Bi et al. [25] found based on fMRI that semantic processing is also more involved in the cognition of abstract concepts of human. Secondly, using valence to understand [23], adults also use this effect to understand abstract concepts [26]. The third factor is the involvement of visual processing, as Amedeo D’Angiuli et al. [27] found that as abstract words are deeply processed, occipital lobe activity is significantly activated, indicating that children are likely to begin “imagining” the word.
However, for drawings, it is not entirely the same, as this approach clearly requires children to react and express their understanding of abstract concepts (i.e. they need to demonstrate it through behaviors), which is significantly different from the results obtained by directly scanning the brain using neuroscience equipment mentioned earlier. The previous review showed that there are two tendencies in children’s drawing toward abstract concepts: one is influenced by language and the other is to search for prototypes that can represent this abstract concept (which seems to be similar to the visual processing mentioned earlier, as finding prototypes is a concrete process that clearly requires imagination [27]) [21].
We also found these two clear tendencies in this study, and we found that among the nine themes/concepts we explored, one theme clearly belongs to the abstract category: Physical and chemical changes (obviously, these two terms represent two major types of changes, rather than two concrete concepts). Therefore, the following analysis is mainly focused on this theme.
Firstly, as mentioned earlier, regarding the drawing of this theme, children have more or less reproduced the experiments in the classroom 100%. It can be seen that they have signs of searching for prototypes, because obviously the experiments once appeared in the classroom, which can naturally become typical exemplars for establishing prototypes of “Physical and chemical changes.”
Secondly, the tendency to use language to represent this abstract theme is also very evident. Data shows that in 160 children’s drawings, 53.8% of them contain at least one of prompts of “physical changes” and “chemical changes,” and there are also many drawings that contain both. In addition, 89.5% of these drawings intentionally divide physical or chemical changes into two groups (left in Fig. 27). More representative is that 13.1% of the drawings do not have any graphics at all, but instead fully utilize mind maps to present this theme (right in Fig. 27). This seems to reflect that when students find it difficult to represent a concept with concrete graphics, they tend to use textual explanations as a concise and intuitive way to solve this problem. The data for judging the drawings mentioned above comes from two researchers who independently assessed three indicators of the drawings: Whether the drawing contains prompts for “physical changes” or “chemical changes”? Is the drawing intentionally grouping examples of the two changes? Is this drawing a graphic or a mind map? The Kappa coefficient is equal to 0.93 (95% CI [0.88, 0.96]), indicating a very high consistency rate between the two individuals’ judgments and a high level of credibility in the results.
Left: using pictures to represent this abstract theme, and dividing the pictures into two groups: physical changes (left) and chemical changes (right); right: using mind maps to represent this abstract theme
What we are most concerned about is undoubtedly the factor of “focus points,” because the results presented in this article do not seem to be completely consistent with the ideas advocated by international science education at present.
Undoubtedly, in the field of science education today, it is the mainstream to advocate teaching more implicit abilities such as reasoning and critical thinking in science classrooms, rather than just allowing students to memorize and apply concrete knowledge. Both Bloom’s theory [28] and internationally renowned literature on science education such as NGSS [29] or PISA [30] place great emphasis on these implicit abilities. In other words, the lessons advocated by science education should not only focus on knowledge and inquiry activities themselves, but should also encourage students to infer the true results from these specific activities—i.e. “what these experimental results indicate/prove.”
However, from Piaget’s perspective, can school-age children really handle so many learning tasks that contain implicit components? No later than 1962, Jean Piaget published his famous theory of cognitive development stages in children [31]. In this theory, Piaget classified the cognitive development of children aged 7–12 years into concrete operations stages (COS). Piaget believed that although children in this stage have more mature abilities such as classification and decentralization compared to the previous stage (pre-school), their thinking is still limited to concrete things that are real or imagined, and they lack the ability to use abstract ways of thinking [20]. Although this theory has received some questions [32], it remains one of the most important theories in developmental psychology. And this theory is also supported by some neuroscience evidence, such as the developmental process of default networks [33, 34]. Especially H.M. Dong et al. [35] proposed that there is a sharp turning point in the development process of this network around the age of 13 years, which promotes the transition of cognitive processing from concrete based to abstract based]. And at the age of 13 years, it happens to be very close to the end point of the COS stage.
Our results have almost fully validated this theory. As shown in the children’s drawing of the “Boiling” concept, compared to the changes in the state of water during heating, children are obviously more concerned about experimental instruments such as beakers and alcohol lamps. This is not difficult to understand, and experimental equipment is more concrete and easily accessible than state changes. And “Increasing the carrying capacity of a boat” is more typical and interesting—the children seem to have completely forgotten that this lesson should have had two items, and instead focused all their attention on the more concrete content of making a small boat, rather than adjusting the variable (carrying capacity). Their brains seem to have unintentionally “declined” this implicit task.
However, we cannot completely avoid abstract and implicit learning tasks in science class, so what should we do? A simple approach is to emphasize learning goals, especially implicit goals, to students before learning. Another approach is to modify the teaching process, reducing concrete content and emphasizing abstract content more (such as directly providing a small boat for students and using most of the class time to adjust the load capacity). But when we sought advice from frontline teachers, we received a more interesting suggestion: try to concretize abstract tasks. Many teaching methods can help achieve this idea, such as STEAM [36]. In addition, the development of computer technology has given this idea more possibilities, such as using visualization methods to visualize abstract mathematical solving processes [37]. Especially with the development of artificial intelligence technology (especially LLM), learners can have timely conversations with LLM in the classroom and receive timely feedback through text and images, which helps to visualize abstract tasks [38]. The exploration of above has received extensive teaching and academic discussions in China [39].
Of course, due to the limitations of researchers’ own abilities, the number of drawings we can process is not yet sufficient to establish a norm that covers all themes/concepts in science education. But we believe that the above results can provide some reference value for children’s drawing research and reveal some cognitive rules of children toward scientific concepts. We also hope that more researchers can further develop the methods in this article to analyze more children’s drawings, in order to truly establish a norm that covers all themes/concepts.
In addition, there are three shortcomings in this study: firstly, all student participants are from Beijing, China, and most participants use the same version of textbooks, which undoubtedly limits the universality of the conclusions of this study in different educational and cross-cultural backgrounds. Therefore, future researches will strive to make up for this deficiency by including participants from different regions and educational environments (e.g. students in underdeveloped areas and who use different versions of textbooks), in order to increase the universality of research conclusions. Secondly, The high P-value and low ICC (3,1) value indicate of the Likert scale that we have not actually found any relevant factors for the representation of children’s drawings in abstract concepts, suggesting that this issue may be more complex than we imagine. Given that this section is not the main focus of this article and we are not experts in studying abstract concepts, this question can only be left to other interested researchers to answer. Thirdly, due to limited manpower and energy of the research team, we only examined nine concepts/themes. This is also the main reason why we use Kendall’s τ for correlation coefficient calculation. However, such a small sample size inevitably reduces the reliability of the results. Therefore, it is recommended that readers consider the results in the “Analysis of related factors” section of this article as a reference only.
This article provides a novel approach to understanding children's drawings, which utilizes LLM to convert drawing information into more accurate linguistic information, allowing for in-depth research on children's conceptual learning using linguistic tools (such as NLP algorithm). The experimental results indicate that, the representation of most drawings has consistency, manifested as most semantic similarity >0.8. And it was found that the consistency of the representation is independent of the accuracy (of LLM's recognition), indicating the existence of consistency bias. The important reason for this phenomenon is the bias between classroom experiments and teaching purpose, many students focus more on the experiments themselves rather than what they explain. In addition, most children tend to use examples they have seen in class to represent more abstract themes/concepts, indicating that they may need concrete examples to understand abstract things.
========================================
PMC ID: 12380447

Pseudomonas aeruginosa is a gram-negative bacterium which can infect all eukaryotes, including higher vertebrates and plants. For humans, P. aeruginosa is an opportunistic pathogen that infects primarily hospitalized and immunocompromised patients (Mathee et al. 2008, Jeukens et al. 2019, Spagnolo et al. 2021). Recent studies have reported that P. aeruginosa is the most common cause of nosocomial pneumonia and other hospital acquired infections like, urinary tract or surgical-site infections resulting in a significant number of deaths (Litwin et al. 2021). In the WHO Bacterial Priority Pathogens List, 2024, P. aeruginosa is kept under the “High Group” (World Health Organization 2024). Drug resistant bacteria under this group are significantly difficult to treat, cause a substantial disease burden (mortality and morbidity), show increasing trends in resistance, are difficult to prevent, are highly transmissible and there are few potential treatments in the development pipeline. Although they may not be critical globally, pathogens in this category could be critical for some populations and in specific geographical areas (World Health Organization 2024).
Conventional bacterial antibiotic susceptibility testing methods (AST) are the most popular methods to determine the antibiotic susceptibility of a bacterial pathogen. AST is the gold-standard method of antibiotic resistance profiling. However, an AST typically requires 48-72 hours to complete because it involves culturing bacteria and detecting drug resistance by either the disc-diffusion method or by conducting an assay to probe the presence of a specific chemical compound (Benkova et al. 2020, Gajic et al. 2022). On the other hand, molecular diagnostic methods like PCR are rapid and sensitive but cannot provide reliable results independently due to their limited specificity (Kurkela and Brown 2009, Pan et al. 2022, Wang et al. 2022, Liu et al. 2023). These methods may lack specificity as they often target genes or specific genetic markers only, requiring additional tests for accurate results (Yamin et al. 2023). Thus, these are not feasible for rapid testing on a mass scale and there is a pressing need to develop innovative and accurate solutions to discern the antimicrobial resistance (AMR) profile of a pathogenic microbe (Gajic et al. 2022). Deciphering the AMR profile of a bacterium using the traditional sequence alignment-based approaches is limited to known data. Hence, whole genome sequencing-based methods can be an alternative approach for predicting the resistance phenotype of a bacterial pathogen (Gupta and Verma 2019, Köser et al. 2014, Vanstokstraeten et al. 2023). Whole genome-based methods have also become promising alternatives due to a continuous decline in genome sequencing costs. Both factors have resulted in the availability of a large volume of genomic data.
The abundant availability of whole genome sequences offers a promising approach for resistance phenotype prediction through machine-learning methods that can be used as an alternative to the traditional sequence alignment-based methods. Prediction of antibiotic resistance profile using genomic sequences and machine-learning has been successfully implemented in many bacterial pathogens like Escherichia coli and Klebsiella pneumoniae (Sakagianni et al. 2023).
In this study, we have described a comprehensive pipeline for the prediction of antibiotic resistance in P. aeruginosa using a two tier machine learning framework based on k-mer analysis. In the first tier, the model predicts resistant or sensitive phenotypes of P. aeruginosa and in the second tier it predicts the specific antibiotic against which resistance has been reported. We evaluated various machine learning algorithms and the results of the best model were further interpreted through explainable AI techniques. The model has also been validated on an independent dataset of P. aeruginosa and the pipeline holds potential to be applied on other pathogenic bacteria as well.
In this study, we tried to address two problems simultaneously; hence, our model works in two tiers. Firstly (referred to as Tier-I), it detects if the given genome belongs to antibiotic-resistant or susceptible P. aeruginosa. If the genome is predicted to belong to an antibiotic-resistant P. aeruginosa, the Tier-II prediction comes in, which predicts the antibiotic against which the P. aeruginosa was resistant.
AMR phenotype data and corresponding genome sequences for P. aeruginosa isolates were retrieved on 6 June 2023, from the Pathosystems Resource Integration Center (PATRIC), part of the Bacterial and Viral Bioinformatics Resource Center (Olson et al. 2023). PATRIC provides detailed annotations for each isolate, including resistance phenotype (Resistant/Susceptible), type of evidence (Laboratory or Computational), and the specific antibiotics linked to resistance. For this study, we filtered the dataset to include only those isolates whose AMR information was supported by laboratory-based evidence (Fig. 1). Among the 18 438 P. aeruginosa isolates listed in the AMR phenotype section of PATRIC, the AMR phenotype of 7708 P. aeruginosa isolates was inferred using computational methods, while for 865 isolates, the source of AMR phenotype annotation was unknown. Laboratory-confirmed phenotype data were available for only 9865 isolates. Of these, 2863 isolates were categorized as resistant, 4505 as susceptible and 533 as intermediate. The AMR phenotype for the remaining 1964 isolates was not reported or remained unknown. Since the aim of Tier-I was to determine whether a P. aeruginosa isolate was antibiotic-resistant or susceptible, any organism showing resistance to even a single antibiotic was classified as resistant. Based on this criterion, we identified 732 P. aeruginosa isolates that were reported by PATRIC, as resistant to at least one antibiotic. Furthermore, an isolate was classified as sensitive only if it showed susceptibility to all the antibiotics considered in this study. Based on this criterion, we identified 372 unique isolates that met the condition and were therefore labeled as sensitive.
Preparation of input dataset for first layer of the machine learning model.
The aim of Tier-II was to predict the resistance or susceptibility of P. aeruginosa to specific antibiotics. Consequently, the approach to data preparation was modified accordingly. The selection of antibiotics commonly used to treat P. aeruginosa infections was adopted from the study of Ibrahim et al. (2020). From this list, we included only those antibiotics for which the PATRIC database contained resistance data from at least 100 P. aeruginosa isolates (Table 1, available as supplementary data at Bioinformatics Advances online).

Generation of k-mers: The genomic data of P. aeruginosa was encoded into k-mers using the Phenotypeseeker tool (Aun et al. 2018). To generate the k-mers, we utilized the GenomeTester4 software suite (Kaplinski et al. 2015), which offers a range of tools for extracting k-mers of specified lengths from genomic datasets. Specifically, we employed the GListMaker utility from GenomeTester4 to create k-mer profiles of all the genome samples. We generated binary k-mer lists for lengths 3, 8, 13, 18, 23, and 28 using a sliding window approach. However, codon boundaries were not incorporated while generating k-mers.

Matrix construction: After generating the k-mers, we created a binary matrix showing the presence or absence of each k-mer in each genome of P. aeruginosa. The presence/absence matrix was then used for statistical testing to find the association of k-mers with the phenotype.
Let M represent a matrix and Mij denote an entry in the row i and column j of that matrix. If a specific k-mer kj is present in a genome sample i, then the value of Mij will be 1; if it is absent, the value will be 0.
The matrix construction can be represented by the following equation:
where M is the presence/absence matrix, i indicates the genome samples, and j indicates k-mers.

Feature selection: Selecting the most informative features is a critical step in developing an effective machine learning model. To identify relevant features, k-mers in the present case, we assessed the significance of different k-mers with the AMR phenotype. As part of this process, we first estimated the genomic distances between genomes of different antibiotic resistance phenotypes using Mash (Ondov et al. 2016). These distance estimates were then used to compute phylogeny-based weights with the help of the PyCogent python library (Knight et al. 2007).

Testing the association of k-mers with the drug-resistance phenotype: To determine the association of k-mers with drug-resistance phenotypes, a χ2 test was performed using the scipy.stats python package (Virtanen et al. 2020). During the chi-square test the null hypothesis assumed that there was no association between the k-mers and phenotype. In contrast, the alternative hypothesis assumed that the drug-sensitive and drug-resistant phenotype was associated with a specific pattern of k-mers.
Let χ2 denotes the chi-squared statistic. The null hypothesis H0 assumes no association between the k-mers and the binary phenotype whereas the alternative hypothesis H1 assumes that there is an association between the two. The test can be represented by the following equation:
where Oi represents the observed frequency of k-mer i in a specific phenotype, Ei represents the expected frequency for that specific phenotype under the null hypothesis.
To build the prediction model, we used three different machine learning algorithms, namely, support vector machine (SVM), decision trees (DT), and logistic regression (LR) with different k-mer lengths and evaluated their performance. All machine-learning algorithms were implemented using the python library, Scikit-learn (Pedregosa et al. 2011). After the initial screening of features explained in the Feature selection and optimization, we used only statistically significant k-mers for building the model, that is, only those k-mers whose P value was less than .05.
To select the number of statistically significant k-mers for our classification model, we performed lasso regression analysis to get 0.01%, 0.1%, 1%, and 5%. We performed this analysis with a maximum of 0.01%, 0.1%, 1%, and 5% lowest P value k-mers in different sets. During training, the datasets were split into training (75%) and test (25%) for each set. The proportion of class labels in the training and the test sets was kept the same as it was in the original, undivided dataset. The plots were made using the ggplot2 R library (Wickham 2016).
We evaluated the performance of various machine learning classifiers using metrics such as sensitivity, specificity, Matthews Correlation Coefficient (MCC) and the area under the receiver operating characteristic curve (AUC-ROC). Sensitivity, the true positive rate of prediction, was defined as:
Specificity, that measures the true negative rate of prediction, was defined as:
The AUC-ROC represents the area under the receiver operating characteristic curve and indicates the model’s ability to distinguish between different classes. Its values range from 0 to 1, where 1 shows the perfect classification and 0 indicates no discriminatory power. These evaluation metrics provide an objective and unbiased means to assess the performance of a given classifier (Eng 2005, Fawcett 2006).

SHAP analysis: To understand the contribution of individual k-mers to the performance of the best-performing model, we used SHapley Additive exPlanations (SHAP) analysis. SHAP quantifies the impact of each feature on the model’s predictions (Chen et al. 2022). The SHAP explainer was constructed using the same datasets and parameters that were applied during model training and development. The resulting SHAP values for different k-mer lengths were analyzed to evaluate the influence of k-mer size on model performance and to identify the k-mers that contributed most significantly to the predictive power of the model.

Independent dataset: We validated our prediction model using an independent dataset from the AMRFinderPlus (version 3.11.11, database version 2023-04-17.1) of National Center for Biotechnology Information (NCBI) (Feldgarden et al. 2021). From AMRFinderPlus we selected only those genomes that were absent from the PATRIC repository at the time of retrieval (6 June 2023), ensuring their independence from the training data. The independent dataset consisted of 56 P. aeruginosa genomes, comprising 47 resistant and 9 susceptible isolates.
For model building, we first applied χ2 function to filter k-mers, retaining those with a P value <.05. From these, the top 1000 lowest P-valued k-mers were selected and used as features for different machine learning classifiers, namely, SVM, LR, and DT, to develop Tier-I prediction models. For LR model k-mer length 13, the maximum mean accuracy, sensitivity, specificity, and AUC-ROC values were 79%, 86%, 64%, and 0.75, respectively (Table 1). The obtained mean accuracy, sensitivity, specificity, and AUC-ROC values for the DT model k-mer length 13 were 79%, 84%, 70%, and 0.77, respectively (Table 1). The maximum mean accuracy, sensitivity, specificity, and AUC-ROC value of the SVM model of k-mer length 13 were 71%, 93%, 29%, and 0.61, respectively (Table 2, available as supplementary data at Bioinformatics Advances online). Our DT model with k-mer length 13 showed the most optimal performance. All reported performance metrics correspond to the independent 25% test set obtained from the random training-test split.
Performance of LR and DT models created using different k-mer lengths for Tier-I (refer Table 2, available as supplementary data at Bioinformatics Advances online, for a comparative assessment).
ROC Curve also showed that the performance of the DT model was better than other models (Fig. 2). We selected the DT model of k-mer length 13 for further analysis.
Plot showing ROC curves of three different machine learning classifiers used to develop Tier-I of prediction schema.
In Tier-II, we developed antibiotic-specific decision tree models using k-mer length 13 (Table 2). Twenty-four antibiotics are commonly used to treat P. aeruginosa infections (Ibrahim et al. 2020). However, of these 24 antibiotics, we developed the prediction model only for 13 antibiotics. The reason was the availability of a very small number of genome sequences of P. aeruginosa in the PATRIC database, which were resistant to the remaining antibiotics. The antibiotics’ class for which we developed prediction models were Penicillins and Penicillin + β-lactamase inhibitor combination (Piperacillin), Cephalosporins (Ceftazidime, Cefepime), Monobactams (Aztreonam), Fluoroquinolones (Ciprofloxacin, Levofloxacin), Carbapenems (Doripenem, Imipenem, and Meropenem), Novel β-lactams and β-lactamase inhibitors (Ceftolozane) and Aminoglycosides (Amikacin, Gentamycin, and Tobramycin). During the training of DT models for the prediction of genomes that were resistant for these 13 antibiotics we kept the k-mer size as 13 and chose the top 1000 k-mers, same as in the Tier-I. As shown in Table 2, the maximum and minimum prediction accuracy was obtained with Imipenem and Aztreonam, which were 0.98 and 0.71, respectively. Antibiotics class-wise analysis revealed that the maximum accuracy was achieved in Piperacillin 0.98 followed by Carbapenems. Among the three antibiotics that were part of class Carbapenems, maximum accuracy was obtained in Imipenem (0.98) followed by Doripenem (0.95) and Meropenem (0.86). In the three classes of Aminoglycosides namely Amikacin, Gentamycin, and Tobramycin, we obtained a similar accuracy of 0.91-0.93. In antibiotic class Fluoroquinolones, Ciprofloxacin showed comparatively better accuracy of 0.92 than the Levofloxacin where accuracy was 0.85. Additionally, we evaluated how the number of top ranked k-mers, selected on the basis of lowest P value, affected the model’s performance. For this we used DT classifier at k-mer length 13 and developed models using top 0.01%, 0.1%, 1%, 5%, and 10% lowest P value k-mers both for Tier-I (Table 3) and Tier-II (Table 2, available as supplementary data at Bioinformatics Advances online). All DT models developed earlier, were using the top 1000 k-mers which is the default mode of learning in Phenotypeseeker (Aun et al. 2018). We found that top 1% k-mers of DT models developed using k-mer length 13 yielded the most optimal performance for Tier-I (Table 3). We observed a gradual increase in the model’s performance as the top k-mers were increased from 1% to 5% for most antibiotics at our Tier-II (Table 2, available as supplementary data at Bioinformatics Advances online). This variation in behaviour of model performance at Tier-I and Tier-II indicates that with the increase in specificity of dataset more significant k-mers are available for model construction.
Performance of DT model used on antibiotics in the Tier-II of our resistance prediction model with k-mer length 13.
Performance of DT models of k-mer length 13 developed by selecting different top k-mers (lowest P value) for Tier-I.
The SHAP summary plot discerns the contribution of individual k-mers to the predictions of the trained model. Each point represents a SHAP value for a specific k-mer in a single sample, with pink indicating a high feature value (indicating frequent presence of the k-mer) and blue indicating a low feature value (rare presence or absence). We have constructed such SHAP plots to interpret the findings of our model at both Tier-I (Fig. 3) and Tier-II (Fig. 4). In these plots, the horizontal position of the points reflects the SHAP value—how much that k-mer pushed the model output towards a positive or negative class. For instance, in our SHAP interpretation plot for Tier-I (Fig. 3) k-mers ACGTCGCGCCCCG and GCCATACGGGTAA exhibited a strong negative impact on model output when present in high abundance (pink dots on the left), which suggest their association with the negative class. Conversely, k-mers such as GAAATCCAGATCC and ACTCATCACGAAC possessed SHAP values that shifted towards the positive side when their abundance was higher, indicating a contribution towards the positive class. Some k-mers like TAGTGTCGGGAAA exhibited minimal spread around zero, implying limited contribution to the model’s predictions. A few k-mers, such as CATCTCGCAGTTA, exhibited a wider SHAP value spread, indicating variability in their contribution across samples. The plot reveals that not all high-frequency k-mers were equally informative; their impact depended on both frequency and context within the model. It is also evident that certain k-mers consistently aligned with either positive or negative predictions, supporting their potential biological relevance. Overall, this SHAP plot is a powerful tool for interpreting the k-mer patterns that most strongly influenced the model’s classification decisions. We then interpreted our Tier-II, antibiotic specific DT models of k-mer length 13 (Fig. 4).
SHAP Summary Plot evaluated by the SHAP method and the effects of each feature on the outcome of DT model of k-mer length 13 (Tier-I) Pink color represents high feature value, i.e. strong presence of that particular k-mer in resistant strain if the value is positive and in sensitive strains if the value is negative. Similarly, blue color represents low feature value, i.e. absence of that particular k-mer in resistant strain if the value is positive and in sensitive strain if the value is negative.
SHAP analysis of Tier-II decision tree models trained on 13-mer features for each antibiotic. Pink color represents high feature value i.e. strong presence of that particular k-mer in resistant strain if the value is positive and in sensitive strains if the value is negative. Similarly, blue color represents low feature value i.e. absence of that particular k-mer in resistant strain if the value is positive and in sensitive strain if the value is negative. (a) Piperacillin, (b) Ceftazidime, (c) Cefepime, (d) Aztreonam, (e) Ciprofloxacin, (f) Levofloxacin, (g) Doripenem, (h) Imipenem, (i) Meropenem, (j) Amikacina, and (k) Gentamycin. (Full-size plots for all antibiotics are provided in Figures 1–11, available as supplementary data at Bioinformatics Advances online.)
The SHAP analysis of the DT model developed to predict Piperacillin (Fig. 4a) resistance in P. aeruginosa depicted k-mer ACCGCGGTCTACG with a lot of pink dots on the left side of SHAP plot. This indicated that the k-mer ACCGCGGTCTACG had a strong negative impact on model output when present in high abundance, suggesting its association with the penicillin-sensitive P. aeruginosa. On the contrary, k-mers such as CCAGATTGAGGAG and CACTGGTTGCCGA exhibited SHAP values that shifted towards the positive side when their abundance was higher, indicating a contribution towards the positive class.
In case of Ceftazidime (Fig. 4b), CGAGGATGCGAAC and GGTCACGGGTGAC showed strong negative SHAP values when its feature value was low (blue) and strong positive SHAP values when its feature value was high (pink) indicating that its presence pushed the prediction towards resistance and absence pushed it towards sensitivity. Thereby, making it a strong marker for resistance. On the other hand, for Cefepime (Fig. 4c), only two k-mers (TACTTCCGCTCAA and CGATGAGCTGCTC) showed effect on prediction. Remaining 18 k-mers were neutral in nature and did not show any significant impact on either of the predictions. TACTTCCGCTCAA and CGATGAGCTGCTC showed lower impact on negative prediction when absent and higher impact on positive prediction when present. Therefore, they serve as markers for resistance.
The SHAP plot for Aztreonam (Fig. 4d), depicts only a single k-mer (CTGTGCCGGACAA), playing a significant role in both positive and negative predictions. The SHAP plot also indicated that the presence of this k-mers was related to the drug-sensitivity and its absence with drug resistance. Since, the prediction was dependent on the presence of one k-mer only, it might be the underlying reason for the low predictive performance of Aztreonam resistant P. aeruginosa.
For Ciprofloxacin (Fig. 4e), absence of k-mers GCAACAACGTGGA and GACTCGCCGGCTC could be considered as strong markers of negative prediction. Interestingly, high presence of k-mer CTCTATCGCGGGC is indicative of sensitive P. aeruginosa or negative prediction and absence indicates resistant P. aeruginosa or positive prediction. On the other hand, for Levofloxacin (Fig. 4f) absence of k-mer TCACTGATCCGCA has a high negative impact on the model’s prediction and presence holds a significant positive impact indicating it to be a strong marker of Levofloxacin resistance in P. aeruginosa.
In the case of Doripenem (Fig. 4g), absence of k-mer CGGTCGTGCCTGC had a positive impact on predictions of resistant P. aeruginosa. This indicated its absence in the drug-resistant P. aeruginosa. On the other hand, for Imipenem (Fig. 4h), k-mer CCCGTTTAAAGAG had a strong impact on positive predictions and its absence had a strong impact on negative predictions indicating that it could act as a reliable resistance marker. Similarly, for Meropenem (Fig. 4i), k-mer AGCGACCCCGCTC held a high positive impact on model predictions and its absence had a strong negative impact indicating its reliability as a resistance marker.
Interestingly for Amikacin (Fig. 4j), absence of the majority of top 20 k-mers has a negative impact on model predictions. It indicates lack of distinct resistance markers and scattered presence of significant resistance markers in P. aeruginosa. On the other hand for Gentamycin (Fig. 4k), presence of k-mer CGGACCCGGGTCA had a negative impact on model predictions indicating its presence in sensitive P. aeruginosa. Whereas, presence of GGTCGCCATCGAA had a positive impact on model predictions indicating its presence in resistant P. aeruginosa. The SHAP plot for Tobramycin could not be obtained.
We validated the Tier-I of our model using an independent dataset. Of the 47 resistant genomes tested, all were correctly identified as resistant and of the 9 susceptible genomes tested, 8 were identified as susceptible. Overall, we got an accuracy of 98% in our validation. We could not validate the Tier-II models because the genomes retrieved from NCBI-AMR Finder were not annotated with the information about the specific antibiotics.
We further mapped the top-20 k-mers that were labelled as the most significant by SHAP analysis in determining the output of the trained models to the genomes that we used to train the model (Table 4, available as supplementary data at Bioinformatics Advances online). In Tier-I out of 20 k-mers, only 12 k-mers were aligned to the genic region. In Tier-II the number of aligned k-mers range from 7 to 15 for various antibiotic specific models. The aligned k-mers were mapped to genes that belong to virulence factors, heavy metal processing, and other vital processes.
Reports of chronic infections in humans, classification as a priority one pathogen by WHO and development of multidrug-resistant strains has multiplied the attention on P. aeruginosa. The identification of resistance phenotypes is an essential prerequisite for all clinical procedures but traditional methods for detecting AMR are often time-consuming and limited in scope. The development of data backed machine learning based drug-phenotype prediction methods are not only essential but also indispensable in this scenario given the growing global threat posed by antibiotic resistant strains, particularly P. aeruginosa. In this work, we have reported the development of an interpretable machine learning tool using k-mers of 13 nucleotides. The proposed ML framework works in two tiers. Tier-I distinguishes resistant versus susceptible isolates of P. aeruginosa, while Tier-II identifies 13 specific antibiotic(s) associated with resistance. We used three different ML classifiers, namely LR, SVM, and DT. The performance of model was evaluated using parameters like sensitivity, specificity, accuracy, MCC and AUC-ROC Value. Among all three ML methods, the maximum performance was achieved by DT Classifier with a mean accuracy of 79%, sensitivity of 84%, specificity of 70%, and AUC-ROC of 0.77. Validation on an independent dataset was performed for Tier-I with an accuracy of 98% indicates the efficiency and robustness of the model. Targeted resistance prediction and potential utility of the model in clinical or surveillance settings can be improved with further validation of Tier-II models as more data becomes available. In order to understand the relative importance of different genomic k-mers in prediction, we also incorporated SHAP analysis.
This analysis provided the top 20 discriminative k-mers from both Tier-I and Tier-II models, representing k-mers that have the highest impact on model predictions. Genomic mapping of these k-mers revealed associations not limited to annotated AMR determinants, suggesting involvement of genomic elements other than the known antibiotic resistance genes in mediating resistance. While the results demonstrate the utility of k-mer-based prediction of resistance, we acknowledge the inherent limitations of genome-only approaches. We propose that this framework provides a scalable and rapid strategy to augment resistance prediction in P. aeruginosa beyond canonical AMR gene profiling.
========================================
PMC ID: 12380446
Escalating impacts of climate change pose serious threats to ecosystems and agriculture, and thus to human societies and their living conditions. Plants — both wild and cultivated — are constantly facing global warming, drought, and precipitation change, rendering plant adaptability a crucial process. Haplotype-based pangenomics is a collection of genomes from each haplotype in multiple strains of a species, offering a cutting-edge approach for identifying traits that foster climate resilience. Although pangenomics focuses on building a pangenome for an agricultural crop, often within cultivars of a plant species, this technology holds significant untapped potential for preserving biodiversity, stabilizing ecosystems, and sequestering carbon.
Pangenomics has already revolutionized crop breeding, with major breakthroughs achieved in rice, maize, wheat, soybean, tomato, and potato [1], just to name a few. These pangenomic studies have been primarily focusing on crop breeding, adaptation, and evolutionary history [2]. In the context of climate-associated variables, developing a pangenome for non-agricultural species, such as forest trees, wetland plants, and grassland species, is also crucial for water regulation, soil stability, carbon storage, and biodiversity conservation. For instance, forest species such as oak (Quercus) and poplar (Populus) absorb significant amounts of CO2, acting as natural carbon sinks; wetland species such as Spartina alterniflora help prevent floods and store carbon in waterlogged soils; and grassland species in savannas and temperate grasslands have deep-rooted systems that help stabilize soils and store carbon, thereby preventing soil erosion and enhancing soil carbon. Applying pangenomics to these species helps uncover structural variants (SVs) associated with key traits, such as species adaptation and resilience to environmental changes [3,4]. The sexual reproduction genome always originates from a combination of two parental genomes, which may retain a highly heterozygous state. To solve the problem of high heterozygosity, a common strategy is to assemble two parental haplotypes into a chimeric genome. However, this strategy corrodes genomic variations between parents, which may be critical for the hereditability of diseases and phenotypes, and allele-specific gene expression [5,6]. This underscores the need for haplotype-based pangenomics and comparative genomics; the latter emphasizes cross-lineage genomic studies for major crops, such as those among the popular Poaceae and Solanaceae species.
Haplotype-based pangenomics presents a promising approach for analyzing environmental adaptability, as it can capture specific combinations of genetic variations that are crucial for resilience to changing conditions. Previous studies have demonstrated the role of haplotypes in plant climate adaptation and fruit quality heterosis [4,6]. In some highly heterozygous plants, such as trees that are closely related to ecosystems and climate, incorporating haplotypes facilitates the identification of key genes related to environmental adaptation.
A recent study led by Hansheng Zhao built a haplotype-based pangenome from 16 representative moso bamboo (Phyllostachys edulis) accessions (RMAs), providing valuable insights into genetic diversity, genomic architecture, climate adaptation, and population risks under future climate conditions [7]. This is the first pangenome of moso bamboo at the haplotype level, thus providing extensive genomic resources for future bamboo research.
One of the key findings is the extensive genetic variations between haplotypes rather than between accessions (97.0% vs. 3%), which assumed that there has been a significant difference between the two ancestral haplotypes in the common ancestor of moso bamboo populations. Among these RMAs, most inter-haplotype variations (68.5% and 68.8% for short variations and SVs, respectively) are detected across all accessions, while only a few inter-accession variations are present in all accessions (0.2% and 0.9% for short variations and SVs, respectively). Further analysis reveals that the number of single nucleotide polymorphisms (SNPs) among the same haplotypes is significantly lower than that among different haplotypes (637808 SNPs vs. 2933471 SNPs on average) by comparison among haplotypes of the three well-phased genomes.
To systematically compare genes and alleles of the 32 haplotype assemblies, the authors categorize their gene sets into core, softcore, dispensable, and private groups by their presence across accessions (called gene frequency), or into double-allele, single-allele, and variable-allele groups based on allele composition. The 12 groups, resulting from the combination of gene frequency and allele composition, possess different characteristics in terms of gene structure, expression pattern, and functional features. The core and double-allele gene sets have greater gene length, cDNA length, CDS number, and CDS size than the private and single-allele gene sets. Moreover, the average gene expression status gradually decreases from core to private gene sets in all allele composition groups, and gradually decreases from double-allele to single-allele gene sets in all gene frequency groups except for the private gene sets. Furthermore, the core and single-allele gene sets, which represent genes present in all accessions but only in one haplotype assembly, are related to environmental adaptations, such as stress tolerance, disease resistance, and DNA damage repair. These adaptation-related genes are also very important and exploitable in other species.
Allele-specific expression gene sets have a high degree of accession specificity (81.8% in 1–2 accessions vs. 0.1% in all accessions), exhibit tissue-specific expression patterns (72% variable tissue expression patterns vs. 28% consistent tissue expression patterns), and are associated with environmental adaptations, such as stimuli and defense responses and tissue-specific developmental processes.
The genotype–environment association analysis identifies 1050 adaptive variations associated with bioclimatic variables, revealing a greater contribution of climate effects than that of geography (35% vs. 13% of adaptive variations). The risk of non-adaptedness analysis and gradient forest modeling indicate that the northern and western regions may have been facing adaptive risks under the high-emission scenario.
However, current genomic offset models — used to predict how species will respond to climate changes — are limited [8]. These models often fail to account for epigenetic factors (changes in gene activity not caused by changes in DNA sequences) and complex interactions between genes and environments. To overcome these barriers, future models must integrate multi-omics approaches (such as population genomics, transcriptomics, epigenetics, metabolomics, and microbiomics) and adjust to changing environmental conditions (both present and future).
In addition, the application of pangenomics to climate adaptation poses significant technical challenges. First, the large genomes of many plants make sequencing expensive and resource-intensive. Next, current pangenome methodologies largely rely on linear genome models, which pose challenges for integrating multiple genomes from the same species and ensuring accessibility of the integrated representation to biologists. Furthermore, many types of genomic resources, such as haplotype information critical for understanding climate adaptation, remain underutilized or are wasted in pangenome analyses due to technical limitations [7]. This emphasizes the need for the development of new analytical techniques and computational methods to more accurately represent and analyze pangenomic data.
Although promising, the application of pangenomics in climate adaptation poses several ethical and socioeconomic challenges. For example, intellectual property concerns exacerbate inequalities by restricting access to climate-resilient technologies in developing nations. Smallholder farmers in sub-Saharan Africa may struggle to afford drought-resistant seeds due to high costs and patent restrictions. The Nagoya Protocol, which aims to ensure fair benefit-sharing with indigenous communities, provides a framework for equitable access. However, more must be done to enhance fair benefit-sharing mechanisms and to ensure equitable technology distribution. Expanding global funding and supporting subsidized research will help bridge the gap between technological innovation and global food security [9].
Artificial Intelligence (AI) and Machine Learning (ML) are transforming genomic analyses by accelerating the discovery of complex traits required for climate adaptation. AI models are increasingly used to identify SVs and gene–environment interactions [8,10]. When integrated with haplotype-based pangenomes, AI and ML enhance the ability to discern intricate genetic patterns and predict phenotypic outcomes under varying environmental conditions [7]. Additionally, advanced techniques, such as HiFi sequencing, combined with SV analysis, improve the detection of large genomic changes, thereby deepening our understanding of adaptive traits. However, high costs remain a barrier, particularly for developing countries.
Modern-day genome editing tools, such as CRISPR-Cas9, have been extensively explored to understand and develop stress-tolerant plant varieties to mitigate the negative effects of climate changes. CRISPR-Cas9 has frequently been employed to understand the stress-responsive genes of heat, drought, salt, and heavy metals, which can be used to enhance climate resilience [11]. Genome editing tools also provide solutions and strategies for plant conservation or restoration, although there are challenges, such as obtaining efficient transformants, producing transgene-free plants, achieving homozygous mutants, and assessing ecological risks [12].
The success of the pangenome depends on global collaboration, open-access data, and continued integration of AI and gene-editing technologies. Projects such as the Earth BioGenome Project and the Consultative Group on International Agricultural Research are already promoting cross-border sharing of genomic data, allowing researchers to apply insights where they are most needed. Reducing costs and expanding data-sharing efforts are essential to ensure that AI-enhanced genomic tools can be effectively leveraged for global climate resilience.
Looking forward, the convergence of long-read sequencing, AI, and CRISPR is set to revolutionize agriculture and biodiversity conservation. Pangenomics based on long-read sequencing can identify genomic variations related to key traits, AI-based genomic analyses can uncover genomic variations associated with agronomic traits and climate changes, and CRISPR-based genome editing tools can accelerate breeding improvements for environmental adaptation. By fostering global cooperation and making genomic technologies accessible, the scientific community can help create climate-resilient species and protect critical ecosystems.
In conclusion, haplotype-based pangenomes are a promising solution for climate adaptation challenges. By extending genomic tools beyond agriculture to include keystone species that are critical for carbon sequestration and ecosystem stability, climate resilience can be built across multiple fronts. Through global collaboration, integration of AI and precision breeding, and addressing ethical and socioeconomic barriers, we can ensure that genomic innovations benefit all regions. With sustained investments in data-driven research, plants — and the ecosystems they support — can be equipped to thrive in a rapidly changing world.

Wanfei Liu: Writing – original draft, Writing – review & editing. Peng Cui: Conceptualization, Funding acquisition, Writing – original draft, Writing – review & editing. Both authors have read and approved the final manuscript.
Both authors have declared no competing interests.
========================================
PMC ID: 12380407
The normal gait pattern is acquired during growth. Toe-walking is generally regarded as the absence or limitation of heel strike during the initial contact phase of the gait cycle [1-3]. This condition is considered a common and normal deviation until the age of 3 years; however, to keep this gait pattern beyond that age could be pathological, and a specialist evaluation should be considered [4-7].
Toe-walking is a feature present in several neuromuscular conditions, such as cerebral palsy and muscular dystrophy, as well as behavioral disorders such as autism spectrum disorder [5]. However, it can be the result of a simple contraction or shortening of the lower limb muscles, specifically the hamstrings and triceps surae. Idiopathic toe walking (ITW) is a condition of unknown etiology. Currently, ITW is a diagnosis that can only be reached once other causes for a similar pattern of gait have been excluded, mainly cerebral palsy [89], muscular dystrophy [10], neuropathy [11], or foot malformation [12]. ITW represents a common referral to different specialists, and the prevalence was reported to be between 5% and 12% in the pediatric population, without gender differences [7].
A comprehensive assessment of children with ITW involves a detailed physical examination and observational gait analysis. This includes evaluating the range of motion (ROM) of lower limb joints, muscle strength, and performing muscle shortening tests, along with assessing the rotational profile of the lower limbs. This approach is essential to support a clinical diagnosis and to decide the proper and most effective individualized treatment for children with ITW. Electromyography, instrumented gait analysis [413], or the validated “Toe Walking Tool” can be used to support the presence of ITW [1415].
The treatment options for ITW are based on age and the presence of limited ankle ROM due to muscle shortening. Nonsurgical treatments, stretching exercises, and orthotics are the main treatment options for children who have persistent toe walking with normal ankle ROM. However, if decreased ankle dorsiflexion flexibility exists, the treatment options are intended to improve the ankle ROM using serial casting, botulinum toxin, or surgery followed by orthosis to maintain the improvement and support gait training [1617]. The 2019 Cochrane systematic review evaluated the effects of various interventions for ITW, including both conservative and surgical treatments. While the review aimed to assess improvements in ROM, it concluded that there were no significant differences in ROM between treatment groups due to the very low certainty of the evidence [18]. Physical exercise and stretching programs are designed to improve the flexibility and strength of lower limb muscles, improving the ankle ROM and reaching a normal gait pattern. Stretching exercises could contribute to maintaining the ROM obtained after surgical treatment [16]. Exercise and stretching programs can therefore be an effective and noninvasive option for the treatment of ITW.
In recent years, there has been a great expansion of health interventions mediated through new technologies. This has been called eHealth, and it includes the use of electronic technologies and digital communication to improve health care delivery. Mobile health (mHealth) interventions have emerged to facilitate and simplify access to medical care. They allow the collection of patient care–relevant data in real time and the use of this information to monitor, diagnose, and treat patients [19]. There are different mHealth interventions that contribute to the management of certain pathologies in pediatric rehabilitation programs, such as tools developed for parents and caregivers for children with cerebral palsy [172021]. These apps should be developed under the participation, advice, and supervision of health care professionals to ensure the proper functioning for the benefit of both patients and health care systems [21].
In the pediatric population, loss to follow-up is common in children with ITW [22]. One of the primary challenges reported by therapists is low adherence to home exercise programs after the conclusion of physiotherapy sessions. This observation is supported by clinical practice experiences and discussions among specialists, underscoring the necessity for innovative tools such as the Active-Feet app to enhance adherence and support long-term therapeutic outcomes. There is a growing body of evidence supporting examples where physiotherapy has benefited from digital health technologies, primarily existing in the management of musculoskeletal [23] or neurological issues [2425]. Specific exercise tools, virtual reality, and gaming have positively influenced rehabilitation [26]. Telerehabilitation focused on children with ITW is supported by the concept that this population has grown up in a technological environment and that mobile devices are a common way for them to interact with others.
A mHealth platform specially designed to meet the needs of children with ITW would be an innovative approach to improve both adherence and effectiveness of rehabilitation programs. Furthermore, children using mHealth tools could play an active and central role in treatment and, at the same time, the mobile app would allow daily monitoring of treatment by the health care professional.
This study aims to develop and evaluate an mHealth-based platform for children with ITW. In addition, it tested the feasibility and acceptance of a home-based exercise program using a comprehensive mHealth tool to improve walking in patients with ITW. This study describes the context study, content preparation, and mobile app design, with subsequent evaluation using a self-administered satisfaction questionnaire from the perspective of parents and children.
The reference population for the content development was children aged between 7 to 12 years with a diagnosis of idiopathic toe walking who attended a pediatric rehabilitation outpatient clinic. This study was conducted in three phases: (1) assessment of children with ITW needs for a home-based rehabilitation program (content preparation phase), (2) designing an attractive mobile app for children (app design phase), and (3) evaluating the app from the user’s perspective (app evaluation phase).
The development process, from conceptualization to pilot testing of the Active-Feet app, is outlined in Figure 1.
The original idea of developing a specific app for home-based exercise treatment in children with ITW arose from the monthly multidisciplinary meetings that take place in the pediatric outpatient rehabilitation clinic. A physiatrist, physiotherapist, and occupational therapist were involved in the initial content preparation. There were several reasons to consider a mHealth approach for rehabilitation in ITW, some of which are as follows: (1) an increasing number of referrals of children with ITW for physiotherapy treatment, (2) low post-treatment compliance with home-based exercise programs, despite written instructions on their execution. This observation is supported by clinical practice experiences and discussions among specialists, highlighting the need for tools such as an app to improve adherence, (3) the number of trips to the hospital for children and caregivers becomes disruptive for daily routines, so continuing physiotherapy sessions at the hospital was not an option for many families, and (4) performing repetitive exercises at home is not attractive for many children and could become a problem in the relationship between parents and children.
An exploratory search was carried out by researchers in PubMed, the Apple App Store, and Google Play Store, and no mobile app related to the treatment of children with ITW was found.
Initially, the main features of the Active-Feet platform were discussed, focusing on its primary goal of helping children with ITW adhere to the rehabilitation program. The app should allow health care professionals to view and to track children’s compliance and execution of the exercises. At the same time, the patients could visualize themselves performing the exercises using the device’s own camera. It was also considered important to develop a suitable and visually attractive app design for the pediatric population. The app should be intuitive, easy to use, and simple to access to avoid disuse. In addition, the app should be a communication channel between the physician or therapist and the patient.
In successive meetings of the multidisciplinary team, a consensus was reached regarding the type of exercises recommended for ITW. After reviewing different guidelines and considering that muscle shortening is a frequent finding, posterior muscle chain stretching exercises were chosen. The selected exercises are presented in Figure 2 and include posterior muscle chain stretches while sitting (A1-A3) and standing (F), triceps surae stretches standing (B1-B2) and using stairs (C1-C2), stretching the soleus (D), and one-leg hamstring stretch. Finally, the initial content was evaluated by senior specialists (1 physiatrist, 1 physiotherapist, and 1 occupational therapist) to get expert opinions. The initial content was finalized after applying the experts’ recommendations.
The first meeting with the computer scientists involved in the creation of the app was established. It was decided that the app should include three user profiles: administrator, patient, and health care professional. The health care professional profile (physician or therapist) should include functionality to design a customized exercise protocol for each patient. The health care professional could increase or decrease the number or intensity of sessions, eliminating or changing some of the proposed exercises and reviewing the sessions already carried out. In the same way, communications between children and therapists could be performed using the chat included in the app. The administrator role is fulfilled by both the physician, who registers the child in the app, and the therapist, who selects and adjusts the exercises weekly based on the patient’s progress, including the number of sessions completed and the time dedicated to the rehabilitation program. This collaborative approach ensures both medical oversight and personalized therapeutic guidance.
Once the first design was reviewed, the researchers concluded that the appearance of the app was unattractive for children, so it was suggested to design 3D animated avatars to demonstrate the exercises as tutors. It was agreed that the avatars should meet certain requirements. The proposed characters should be visually attractive; they should bring the attention to the feet, the most important part of the body in this case. Therefore, the feet should be well-defined and subtly larger to highlight their relevance, barefoot to better visualize their movement, but they should wear striking socks to add extra visual appeal to the design.
From this entire process, TOBI and TOE characters emerged, 3D avatars enabling the guiding of the patient with ITW in performing the exercises were designed (Figure 3). At this point, the physiotherapists specialized in carrying out the chosen stretching exercises recorded themselves, and the recordings were used as templates for the avatars’ motion. The exercises performed by the 3D avatars were designed to be simple and at the same time easy to mirror by children.
The platform development process was carried out over five stages, executed sequentially: (1) requirements specification, (2) platform design, (3) platform implementation, (4) platform deployment and alpha testing of the mobile app, and (5) validation and performance testing of the mobile app (beta phase).
Tasks were conducted to gather both functional and nonfunctional requirements, aiming to specify the system’s characteristics in terms of functionality (functional requirements): types of users, types of rehabilitation sessions, how interactions between patients and doctors would occur, and other details. In addition, nonfunctional requirements were addressed, primarily tackling three issues: (1) deployment architecture, (2) the need to enhance usability and ensure the accessibility of the app due to the needs of the end users (young patients), and (3) data protection.
In accordance with the requirements (functional and nonfunctional), a design phase was carried out. As a result of this phase, a list of use cases enumerating all the functionalities that should be present in the platform was obtained.
Furthermore, as a result of designing according to the nonfunctional requirements, it was agreed to use a client-server architecture with a single multiuser mobile app (for patients and doctors) that would display different views depending on the role, as well as another view for the system administrator, which has typical administrative functions. (Figures4  5 and Multimedia Appendix 1). The server is planned to serve as a provider of web services to implement part of the functionalities and as a database for information management and storage.
To ensure usability, the services of a company specializing in mobile app interface design were used. This company provided wireframes for the mobile app designed with a user-centered approach, aiming for maximum usability and accessibility.
The Active-Feet platform comprises two deployment environments: a secure backend hosted on a private server and a mobile app. The backend operates on a virtual machine with robust security and reliability measures. The mobile app was tested on 4 physical devices (2 smartphones and 2 tablets) and 2 virtual devices. Physical devices, such as the Xiaomi Redmi Note 12 and Samsung Galaxy Tab A8, demonstrated superior performance due to their advanced hardware. Conversely, the Samsung Galaxy Tab A7 Lite exhibited the lowest performance, likely due to its limited RAM and internal processes. Virtual devices also performed adequately, showcasing the platform’s adaptability across varying specifications. (More detailed information is included in Multimedia Appendix 2). Overall, the platform proved compatible with low-to-mid-range devices, ensuring broad accessibility and delivering a satisfactory user experience, particularly on devices with greater processing power.
Finally, to ensure access to information, protocols were established so that only a patient and their associated doctor could access the patient’s data. Similarly, doctors cannot view other doctors or patients for whom they are not responsible. Only the administrator can view all information. To achieve this, access control mechanisms were implemented. Furthermore, to ensure secure information exchange, all requests between the client (mobile app) and server are made securely using HTTPS and JSON Web Token.
The platform consists of blocks: the server-side (backend) and the mobile app (frontend). The server-side (backend) was developed using the Python programming language (Python Software Foundation) and the Flask framework (Pallet Projects) for web service creation. MongoDB (Mongo Inc.) has been used as the database management system. On the other hand, the mobile app (frontend) was implemented following the wireframes provided by the design company and implemented for the Android (Google LLC) operating system, compatible with versions 5.0 and later (since 2014).
The backend (server) was deployed on a private server belonging to the University of Granada network, which was publicly accessible and equipped with security certificates. The mobile app was tested on three different devices: Xiaomi Redmi Note 8, Xiaomi Redmi Note 9, and Xiaomi Redmi Note 10. Validation tests were conducted to assess functionalities, and common errors derived from the implementation were corrected.
This stage is currently under development. The evaluation of the direct impact on clinical outcomes is being conducted as a second phase of the study by the research group. These follow-up investigations aim to evaluate the app’s effectiveness in achieving functional improvements in children with ITW. The evaluation of exercise accuracy using the Active-Feet app will be carried out by the therapist through visual monitoring during each session. The therapist would observe the patient’s posture to ensure proper alignment and movement, focusing on potential compensations at the pelvis, hips, or knees. In addition, the therapist will check whether the patient dedicates the correct amount of time to each stretch, typically 15 seconds, as specified in the treatment protocol. This process ensures that exercises are performed correctly and safely.
Exercise intensity will be monitored by progressively increasing the number of weekly repetitions for each exercise. Since patients may experience discomfort due to muscle shortening at the start of the intervention, the progression will be carefully adjusted to allow gradual adaptation and avoid excessive strain. Similarly, frequency measurements will include tracking the number of patient connections to the app, the number of days the patient engages in sessions, and the duration of each session. These combined metrics aim to provide a comprehensive understanding of patient adherence, engagement, and rehabilitation progress.
The final version of Active-Feet app was assessed considering the perspectives of parents and children on the usefulness and attractiveness of the app, as well as the ability to encourage children to follow the home-based exercise program.
Participants for this feasibility and acceptance pilot study were required to meet the following inclusion criteria: diagnosis of ITW, aged between 7- 12 years, and ability to use an Android smartphone (with appropriate parental supervision and support). Twenty selected participants who were available and willing to cooperate assessed the app for 2 weeks. To facilitate the use of the app, a first guided use of the Active-Feet app was carried out face-to-face in the outpatient clinic by a therapist, with a detailed explanation provided to the children and parents.
Specific questionnaires were designed by the researchers (refer to Multimedia Appendix 3) and administered to both parents and children following the 2-week app testing period. The answers from adults were measured based on a 5-point Likert scale, while the questionnaire for the pediatric population was evaluated using a 4-point visual analog scale. To summarize data distributions, the median and IQR (25th-75th percentile) were calculated for all Likert scale responses. After collecting the questionnaires from parents and children, the data were analyzed using SPSS software (version 24; IBM Corp).
Ethical approval for this study was obtained from the Granada Research Ethics Committee (CEI Granada; approval code: 95133). Written informed consent was obtained from all adult participants and from the parents or legal guardians of the children involved in the app evaluation. Children also provided assent prior to participation. All personal data collected during the study were anonymized to ensure participant privacy and confidentiality in accordance with applicable data protection regulations. No monetary compensation was provided to participants for their involvement in the study.
A total of 20 children participated in the study. The median age of the children was 10 (IQR 9-11) years, the median weight was 42.8 (IQR 35.6‐55.7) kg, and the median height was 148 (IQR 138‐160) cm.
Regarding ROM and flexibility measurements, the median plantar flexion was 70° for both the right and left sides, with IQRs of 60°‐75.5° for the right and 60°‐74° for the left. Median dorsiflexion with the knee extended was 0° on the right side (IQR −3.5° to 0°) and 1° on the left side (IQR −5° to 4°). With the knee flexed, median dorsiflexion was 10° bilaterally, with IQRs of 6°‐12° for the right and 8.5°‐13.5° for the left. The Silfverskiöld test was positive in all participants, confirming restricted dorsiflexion with the knee extended. Further details of the measurements conducted on the participants can be found in Multimedia Appendix 4.
Three main sections were considered in each questionnaire: (1) questions about the ease of use or user-friendliness, (2) questions about the impact of the app, and (3) attractiveness of the app and user feedback. All these aspects were addressed in both questionnaires using appropriate language for each group—parents and children.
A total of 20 parents, 1 per child in the study, completed the questionnaire. The ease of the registration process, learning to run the app, and access to different sections were the main questions assessed from parents (questions 1, 2, and 3). Regarding the impact of using the app, a question was addressed to the enhancement of their child’s motivation with the app (question 4). Finally, acceptance and attractiveness were addressed by asking whether the content was appropriate for their children (question 5) and whether the app was helpful for the reconciliation of family life and the rehabilitation process (question 6).
In the questionnaire addressed to the children (n=20), questions were asked about how much fun they found (question 1) and their motivation to do the exercises daily (question 5). They were also asked whether they liked the avatars, as a measure of acceptance and attractiveness (question 2). Regarding the impact of using the app, questions addressed how the avatars helped them do the exercises (question 3) and whether the exercises shown by the avatars were easy to mirror (question 4).
The interaction between children and parents with the app technology was well received by all participants. In general, they were pleased with the quality of the graphical user interface, ease of navigation, and the overall impression of how patients interact with the mHealth system.
The parental question regarding the improvement of the child’s motivation to perform the exercises (question 4) received one of the highest scores, followed by the section addressing the impact of using the app (question 5). Finally, the aspects related to the app’s ease of use (questions 1 and 3) and learnability (question 2) were also rated highly by participants (Multimedia Appendix 5). The median score for the first 5 questions was 4 (IQR 4-4). The item referring to the reconciliation of family life with the rehabilitation treatment of their children (question 6) was highly appreciated by parents, showing a median of 5 (IQR 4-5). The overall mean percentage of parents responding “4” across all parental questions was 72.5%. Only 5% of the parents’ responses scored 3 or lower on the Likert scale.
Children’s responses to questions on user-friendliness and motivation demonstrated positive outcomes. For question 1, which assessed whether performing the exercises was an enjoyable experience, the median score was 4 (very good) with an IQR of 3.25-4. Similarly, for question 5, which evaluated whether children remembered to perform the exercises daily with TOBI and TOE, the median score was also 4, with an IQR of 4-4. Questions regarding the impact of use, the app’s helpfulness in learning (question 3), and the ease of mirroring exercises (question 4) received a median score of 3 (good) with an IQR of 3-4. Regarding the acceptance and attractiveness of the app (question 2), children expressed positive feedback, with a median score of 3 (IQR 3-4). They found the avatars to be delightful and engaging. The overall mean percentage of participants responding with a score of 4 across all children’s questions was 52%. No responses of “Bad” or “Very Bad” were recorded.
The aim of this study was to develop and implement an app for telerehabilitation in children with ITW. The findings of this research suggest that the Active-Feet mHealth-based platform is feasible for children. An attractive and easy-to-use app, designed to help patients perform exercises following the instructions of 3D avatars, was developed during the process. At the same time, the Active-Feet app allows for recording completed exercises, enabling the therapist to track the frequency, intensity, and accuracy of performance through their app profile. Corrections or recommendations via chat between the therapist and patient can improve interaction quality and provide guidance to users in the event of errors. Collecting real-time health care data from patients and using this information to monitor, diagnose, and treat them is one of the main advantages of mHealth tools [19].
The approach of ITW shows a lack of consensus regarding optimal conservative management. A wide variation in treatment across providers and institutions exists. However, stretching exercises are advocated as the core management for ITW [1618]. Mobile apps can incorporate positive behavior change techniques to improve exercise adherence and thus optimize clinical benefits. In children with ITW, loss to follow-up is common [22], and long-term adherence remains challenging. Promoting the reconciliation of daily routines and rehabilitation treatment is important to avoid noncompliance to the treatment. Moreover, telerehabilitation improves patients’ access to rehabilitation services and saves patient time traveling to the hospital or rehabilitation center. Evidence suggests that there are many potential applications to be explored to enhance connectivity of the interdisciplinary rehabilitation team in the pediatric population [27]. All these elements were considered in developing the Active-Feet platform.
Although the evaluation of the Active-Feet app did not use the Mobile App Rating Scale (MARS) used in previous studies, the questionnaires administered to assess the app revealed higher user engagement and satisfaction scores compared to similar mHealth apps. For example, pain-related apps evaluated with MARS achieved an average overall score of 3.17 out of 5, with functionality rated the highest and engagement often scoring lower [28]. In contrast, the Active-Feet app demonstrated strong user satisfaction and engagement among parents and children, highlighting its ability to effectively meet the needs of its target population. These findings emphasize the potential of the Active-Feet app to serve as a valuable tool for enhancing adherence and motivation in rehabilitation contexts.
This study describes the entire process of developing a specific app to perform the rehabilitation treatment for ITW in a complex population. The Active-Feet app showed to be attractive and easy enough to use for children and parents. Currently, there is no other app that has been developed for this condition, so no comparison can be made. The reference for this research was supported and guided by previous home rehabilitation interventions administered from our group of researchers in developing mHealth systems [2930]. This paper assembles the preliminary qualitative and quantitative data on usability and acceptability of the Active-Feet app for conducting the home-based exercise program and to explore potential outcomes of the intervention. The direct impact on clinical outcomes such as adherence to rehabilitation protocols or improvements in ROM was not assessed in this study. Future research should focus on overcoming these limitations by using longitudinal studies with objective measures to assess how effective the app is in improving adherence and functional outcomes in children with ITW.
It is important to consider the impact on special groups of children using digital characters or avatars during rehabilitation treatment and the advantages of these kinds of mHealth systems. It was reported that patients with autism have shown benefits, such as being able to resemble the movements of the avatar [31]; improved the ability to recognize and express basic emotions [32]; and increased the capability for social interactions by practicing verbal and nonverbal behavior in virtual reality environments [33]. Autistic children who exhibit toe walking are generally not classified as ITW, and additional considerations may be needed for their cognitive and behavioral differences. Consequently, while this app is primarily designed for idiopathic toe walkers, it could also serve as a valuable tool for children with toe-walking behaviors associated with conditions such as autism or other motor impairments, provided appropriate adaptations are made. In addition, a recent systematic review of mobile apps for patients with chronic conditions or multimorbidity has called for further research to develop and evaluate apps that are both high quality and have a high capacity to promote positive behavior change in patients [34].
This study had additional limitations. Twenty participants collaborated with the researchers to assess the app. Although some studies suggest that this sample size in usability tests can identify over 80% of usability issues [3536], it may not capture all potential problems. In future research, we should consider a larger sample size and a more extended period of follow-up. The missing information regarding the participants’ previous knowledge of the exercises was another limitation of this study. In addition, the assessment of exercise accuracy relied on visual monitoring by the therapist; however, the lack of quantitative metrics such as joint movement tracking may limit the precision and consistency of exercise evaluation. Future research could benefit from incorporating motion tracking technologies to provide more objective and accurate assessments of exercise performance.
The use of a custom-designed satisfaction questionnaire instead of a standardized usability assessment tool, such as the System Usability Scale (SUS), was also a constraint in our research. While our questionnaire was specifically tailored to suit the linguistic and cognitive needs of a pediatric population, it limits the comparability of our findings with those of other studies. Future studies should consider incorporating validated and widely used scales to enhance the generalizability of results.
Finally, the findings of this study reveal the structure and process aspects of developing and optimizing a mobile app aimed at the pediatric population. However, the implementation of the Active-Feet app is not recommended until feasibility and effectiveness have been demonstrated in appropriately designed randomized controlled trials. This study also has strengths. First, the acceptance and satisfaction of the intervention have been demonstrated by users and parents. Second, the Active-Feet app was developed for both mobile and tablet devices. Third, to our knowledge, this is the first study showing the entire process of a mHealth platform created to deliver telerehabilitation programs for children with ITW.
Exercise and stretching programs can be an effective, noninvasive option for the treatment of ITW in children, and the use of mobile apps is an alternative for home-based treatment programs. Although the Active-Feet app has been implemented in patients with ITW, its use could potentially be extended to children who exhibit toe walking as a frequent finding, such as those with autism spectrum disorders. This study describes the entire process of developing a mHealth system, and an attractive, easy-to-use mHealth platform for children with ITW was developed for the first time. Future studies should evaluate long-term effectiveness of the app and determine whether it should be recommended for patients, relatives, or health care professionals.
========================================
